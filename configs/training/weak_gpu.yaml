# @package _global_

# Training configuration optimized for weak GPUs
training:
  # Basic training settings (reduced for weak GPU)
  epochs: 50
  first_epoch_gmm: 20
  refit_gmm_interval: 30
  lambda_gmm: 1.0
  
  # Optimization settings
  optimizer:
    decoder:
      type: "AdamW"
      lr: 0.001
      weight_decay: 0.01
      
    representation:
      type: "AdamW" 
      lr: 0.01
      weight_decay: 0.01
      
  # Learning rate scheduling
  scheduler:
    type: "CosineAnnealingLR"
    T_max: 50
    eta_min: 1e-6
    
  # Early stopping
  early_stopping:
    enabled: false
    patience: 10
    min_delta: 1e-4
    metric: "total_loss"
    mode: "min"
    
  # Checkpointing
  checkpointing:
    enabled: false  # Disabled for speed
    save_best: false
    save_last: false
    save_interval: 50
    
  # Logging and visualization (reduced for weak GPU)
  logging:
    log_interval: 5
    plot_interval: 10
    save_figures: false  # Disabled to save time and memory
    
  # Validation
  validation:
    enabled: true
    interval: 1
