{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuML: Installed accelerator for sklearn.\n",
      "cuML: Installed accelerator for umap.\n",
      "cuML: Successfully initialized accelerator.\n",
      "PyTorch CUDA version: 12.6\n",
      "PyTorch version: 2.7.0+cu126\n",
      "Number of CUDA devices: 1\n",
      "Device 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "  Memory Allocated: 0.0 MB\n",
      "  Memory Reserved: 0.0 MB\n",
      "  Total Memory: 7805.5625 MB\n",
      "Using device: cuda:0\n",
      "PyTorch CUDA version: 12.6\n",
      "PyTorch version: 2.7.0+cu126\n",
      "Number of CUDA devices: 1\n",
      "Device 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "  Memory Allocated: 0.0 MB\n",
      "  Memory Reserved: 0.0 MB\n",
      "  Total Memory: 7805.5625 MB\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from datetime import timedelta\n",
    "#import torch_optimizer as optim\n",
    "\n",
    "try:\n",
    "    from cuml.accel import install\n",
    "    install()\n",
    "except ImportError:\n",
    "    print(\"Cuml not installed, using CPU for umap and sklearn\")\n",
    "\n",
    "from src import RepresentationLayer, DGD, ConvDecoder, LatentSpaceVisualizer, GaussianMixture, plot_training_losses, plot_images, plot_gmm_images, plot_gmm_samples\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i)/1024**2} MB\")\n",
    "        print(f\"  Memory Reserved: {torch.cuda.memory_reserved(i)/1024**2} MB\")\n",
    "        print(f\"  Total Memory: {torch.cuda.get_device_properties(i).total_memory/1024**2} MB\")\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "use_small_dataset = False\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 60000 (total: 60000)\n",
      "Test dataset: 10000 (total: 10000)\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Image size: 784\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset, use_subset=False, subset_fraction=0.1):\n",
    "        \"\"\"\n",
    "        Wrap a dataset with indices\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset: The original dataset\n",
    "        use_subset: Whether to use only a subset of the data\n",
    "        subset_fraction: The fraction of data to use if use_subset is True\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if use_subset:\n",
    "            # Create a subset of indices (10% by default)\n",
    "            total_size = len(dataset)\n",
    "            subset_size = int(total_size * subset_fraction)\n",
    "            \n",
    "            # Create random indices ensuring we get samples from all classes\n",
    "            all_indices = list(range(total_size))\n",
    "            np.random.shuffle(all_indices)\n",
    "            self.indices = all_indices[:subset_size]\n",
    "        else:\n",
    "            # Use all indices\n",
    "            self.indices = list(range(len(dataset)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Map the index to the original dataset index\n",
    "        orig_index = self.indices[index]\n",
    "        data, target = self.dataset[orig_index]\n",
    "        return orig_index, data, target\n",
    "\n",
    "# Create datasets with the option to use only a subset\n",
    "indexed_train_dataset = IndexedDataset(train_dataset, use_subset=use_small_dataset)\n",
    "indexed_test_dataset = IndexedDataset(test_dataset, use_subset=use_small_dataset)\n",
    "\n",
    "# Adjust batch size if using a small dataset\n",
    "batch_size = 128\n",
    "if use_small_dataset:\n",
    "    # Use smaller batch size for small datasets\n",
    "    batch_size = min(batch_size, max(32, len(indexed_train_dataset) // 10))\n",
    "    print(f\"Using small dataset, adjusted batch size: {batch_size}\")\n",
    "\n",
    "train_loader = DataLoader(indexed_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(indexed_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset info\n",
    "print('Train dataset:', len(indexed_train_dataset), \n",
    "      f'({\"10% of \" if use_small_dataset else \"\"}total: {len(train_dataset)})')\n",
    "print('Test dataset:', len(indexed_test_dataset), \n",
    "      f'({\"10% of \" if use_small_dataset else \"\"}total: {len(test_dataset)})')\n",
    "\n",
    "# Print shape of an image\n",
    "print('Image shape:', train_dataset[0][0].shape)\n",
    "# Print total number of pixels in an image\n",
    "print('Image size:', train_dataset[0][0].numel())\n",
    "\n",
    "all_labels = torch.tensor([train_dataset[i][1] for i in indexed_train_dataset.indices], device=device)\n",
    "\n",
    "# Get a batch of images and labels\n",
    "batch_train = next(iter(train_loader))\n",
    "indices_train, images_train, labels_train = batch_train\n",
    "\n",
    "batch_test = next(iter(test_loader))\n",
    "indices_test, images_test, labels_test = batch_test\n",
    "\n",
    "# Plot the images with their labels\n",
    "plot_images(images_train, labels_train, 'Fashion MNIST Train samples', epoch=None, cmap='viridis')\n",
    "plot_images(images_test, labels_test, 'Fashion MNIST Test samples', epoch=None, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Experiment: uniform_ball_r0.1_epochs200\n",
      "üìÅ Figures will be saved to: figures/uniform_ball_r0.1_epochs200/\n",
      "Decoder parameters: 246,945 (0.25M)\n",
      "Train representation parameters: 300,000 (0.30M)\n",
      "Test representation parameters: 50,000 (0.05M)\n",
      "Total trainable parameters: 596,945 (0.60M)\n",
      "\n",
      "Parameter distribution:\n",
      "Decoder: 45.1%\n",
      "Train rep: 54.9%\n",
      "\n",
      "=== Training Configuration ===\n",
      "Epochs: 200\n",
      "Device: cuda:0\n",
      "First GMM epoch: 50\n",
      "Plot interval: 50\n",
      "Using subset: False\n",
      "Verbose mode: True\n",
      "==============================\n",
      "Initializing GMM at epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asp/Downloads/HeaDS/ImageDGD/src/models/gmm.py:752: UserWarning: EM algorithm did not converge. Try increasing max_iter or lowering tol.\n",
      "  warnings.warn(\"EM algorithm did not converge. Try increasing max_iter or lowering tol.\", UserWarning)\n",
      "/home/asp/Downloads/HeaDS/ImageDGD/src/models/gmm.py:689: UserWarning: EM algorithm did not converge. Try increasing max_iter or lowering tol.\n",
      "  warnings.warn(\"EM algorithm did not converge. Try increasing max_iter or lowering tol.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 [TPE: 0:00:25, RT: 1:23:14]; Train Loss: 69.1948 (0.00%), Test Loss: 68.0370 (0.00%), Recon Train: 69.1948 (0.00%), Recon Test: 68.0370 (0.00%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Unused keyword parameter: max_iter during cuML estimator initialization\n",
      "# of Nearest Neighbors should be at least 3 * perplexity. Your results might be a bit strange...\n",
      "Unused keyword parameter: max_iter during cuML estimator initialization\n",
      "# of Nearest Neighbors should be at least 3 * perplexity. Your results might be a bit strange...\n",
      "build_algo set to brute_force_knn because random_state is given\n",
      "build_algo set to brute_force_knn because random_state is given\n",
      "Epoch 2/200 [TPE: 0:00:23, RT: 1:20:17]; Train Loss: 66.6791 (3.64%), Test Loss: 65.7659 (3.34%), Recon Train: 66.6791 (3.64%), Recon Test: 65.7659 (3.34%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 2/200 [TPE: 0:00:23, RT: 1:20:17]; Train Loss: 66.6791 (3.64%), Test Loss: 65.7659 (3.34%), Recon Train: 66.6791 (3.64%), Recon Test: 65.7659 (3.34%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 3/200 [TPE: 0:00:23, RT: 1:19:20]; Train Loss: 61.2789 (8.10%), Test Loss: 60.8102 (7.54%), Recon Train: 61.2789 (8.10%), Recon Test: 60.8102 (7.54%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 3/200 [TPE: 0:00:23, RT: 1:19:20]; Train Loss: 61.2789 (8.10%), Test Loss: 60.8102 (7.54%), Recon Train: 61.2789 (8.10%), Recon Test: 60.8102 (7.54%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 4/200 [TPE: 0:00:23, RT: 1:18:40]; Train Loss: 55.0315 (10.19%), Test Loss: 54.4429 (10.47%), Recon Train: 55.0315 (10.19%), Recon Test: 54.4429 (10.47%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 4/200 [TPE: 0:00:23, RT: 1:18:40]; Train Loss: 55.0315 (10.19%), Test Loss: 54.4429 (10.47%), Recon Train: 55.0315 (10.19%), Recon Test: 54.4429 (10.47%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 5/200 [TPE: 0:00:24, RT: 1:18:17]; Train Loss: 48.9311 (11.09%), Test Loss: 48.1822 (11.50%), Recon Train: 48.9311 (11.09%), Recon Test: 48.1822 (11.50%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 5/200 [TPE: 0:00:24, RT: 1:18:17]; Train Loss: 48.9311 (11.09%), Test Loss: 48.1822 (11.50%), Recon Train: 48.9311 (11.09%), Recon Test: 48.1822 (11.50%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 6/200 [TPE: 0:00:23, RT: 1:17:45]; Train Loss: 43.7095 (10.67%), Test Loss: 42.7827 (11.21%), Recon Train: 43.7095 (10.67%), Recon Test: 42.7827 (11.21%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 6/200 [TPE: 0:00:23, RT: 1:17:45]; Train Loss: 43.7095 (10.67%), Test Loss: 42.7827 (11.21%), Recon Train: 43.7095 (10.67%), Recon Test: 42.7827 (11.21%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 7/200 [TPE: 0:00:23, RT: 1:17:19]; Train Loss: 39.4941 (9.64%), Test Loss: 38.5495 (9.89%), Recon Train: 39.4941 (9.64%), Recon Test: 38.5495 (9.89%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 7/200 [TPE: 0:00:23, RT: 1:17:19]; Train Loss: 39.4941 (9.64%), Test Loss: 38.5495 (9.89%), Recon Train: 39.4941 (9.64%), Recon Test: 38.5495 (9.89%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 8/200 [TPE: 0:00:24, RT: 1:16:57]; Train Loss: 36.0471 (8.73%), Test Loss: 35.1508 (8.82%), Recon Train: 36.0471 (8.73%), Recon Test: 35.1508 (8.82%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 8/200 [TPE: 0:00:24, RT: 1:16:57]; Train Loss: 36.0471 (8.73%), Test Loss: 35.1508 (8.82%), Recon Train: 36.0471 (8.73%), Recon Test: 35.1508 (8.82%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 9/200 [TPE: 0:00:23, RT: 1:16:30]; Train Loss: 33.3102 (7.59%), Test Loss: 32.2596 (8.23%), Recon Train: 33.3102 (7.59%), Recon Test: 32.2596 (8.23%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 9/200 [TPE: 0:00:23, RT: 1:16:30]; Train Loss: 33.3102 (7.59%), Test Loss: 32.2596 (8.23%), Recon Train: 33.3102 (7.59%), Recon Test: 32.2596 (8.23%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 10/200 [TPE: 0:00:23, RT: 1:16:02]; Train Loss: 31.1067 (6.62%), Test Loss: 30.0693 (6.79%), Recon Train: 31.1067 (6.62%), Recon Test: 30.0693 (6.79%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 10/200 [TPE: 0:00:23, RT: 1:16:02]; Train Loss: 31.1067 (6.62%), Test Loss: 30.0693 (6.79%), Recon Train: 31.1067 (6.62%), Recon Test: 30.0693 (6.79%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 11/200 [TPE: 0:00:24, RT: 1:15:40]; Train Loss: 29.4036 (5.47%), Test Loss: 28.4727 (5.31%), Recon Train: 29.4036 (5.47%), Recon Test: 28.4727 (5.31%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 11/200 [TPE: 0:00:24, RT: 1:15:40]; Train Loss: 29.4036 (5.47%), Test Loss: 28.4727 (5.31%), Recon Train: 29.4036 (5.47%), Recon Test: 28.4727 (5.31%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 12/200 [TPE: 0:00:24, RT: 1:15:18]; Train Loss: 28.0373 (4.65%), Test Loss: 27.0668 (4.94%), Recon Train: 28.0373 (4.65%), Recon Test: 27.0668 (4.94%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 12/200 [TPE: 0:00:24, RT: 1:15:18]; Train Loss: 28.0373 (4.65%), Test Loss: 27.0668 (4.94%), Recon Train: 28.0373 (4.65%), Recon Test: 27.0668 (4.94%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 13/200 [TPE: 0:00:23, RT: 1:14:53]; Train Loss: 26.9779 (3.78%), Test Loss: 25.9418 (4.16%), Recon Train: 26.9779 (3.78%), Recon Test: 25.9418 (4.16%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 13/200 [TPE: 0:00:23, RT: 1:14:53]; Train Loss: 26.9779 (3.78%), Test Loss: 25.9418 (4.16%), Recon Train: 26.9779 (3.78%), Recon Test: 25.9418 (4.16%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 14/200 [TPE: 0:00:23, RT: 1:14:28]; Train Loss: 26.0532 (3.43%), Test Loss: 25.0635 (3.39%), Recon Train: 26.0532 (3.43%), Recon Test: 25.0635 (3.39%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 14/200 [TPE: 0:00:23, RT: 1:14:28]; Train Loss: 26.0532 (3.43%), Test Loss: 25.0635 (3.39%), Recon Train: 26.0532 (3.43%), Recon Test: 25.0635 (3.39%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 15/200 [TPE: 0:00:24, RT: 1:14:07]; Train Loss: 25.2436 (3.11%), Test Loss: 24.2682 (3.17%), Recon Train: 25.2436 (3.11%), Recon Test: 24.2682 (3.17%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 15/200 [TPE: 0:00:24, RT: 1:14:07]; Train Loss: 25.2436 (3.11%), Test Loss: 24.2682 (3.17%), Recon Train: 25.2436 (3.11%), Recon Test: 24.2682 (3.17%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 16/200 [TPE: 0:00:24, RT: 1:13:46]; Train Loss: 24.5270 (2.84%), Test Loss: 23.6536 (2.53%), Recon Train: 24.5270 (2.84%), Recon Test: 23.6536 (2.53%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 16/200 [TPE: 0:00:24, RT: 1:13:46]; Train Loss: 24.5270 (2.84%), Test Loss: 23.6536 (2.53%), Recon Train: 24.5270 (2.84%), Recon Test: 23.6536 (2.53%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 17/200 [TPE: 0:00:24, RT: 1:13:22]; Train Loss: 23.8502 (2.76%), Test Loss: 22.9957 (2.78%), Recon Train: 23.8502 (2.76%), Recon Test: 22.9957 (2.78%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 17/200 [TPE: 0:00:24, RT: 1:13:22]; Train Loss: 23.8502 (2.76%), Test Loss: 22.9957 (2.78%), Recon Train: 23.8502 (2.76%), Recon Test: 22.9957 (2.78%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 18/200 [TPE: 0:00:23, RT: 1:12:57]; Train Loss: 23.2368 (2.57%), Test Loss: 22.2905 (3.07%), Recon Train: 23.2368 (2.57%), Recon Test: 22.2905 (3.07%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 18/200 [TPE: 0:00:23, RT: 1:12:57]; Train Loss: 23.2368 (2.57%), Test Loss: 22.2905 (3.07%), Recon Train: 23.2368 (2.57%), Recon Test: 22.2905 (3.07%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 19/200 [TPE: 0:00:24, RT: 1:12:34]; Train Loss: 22.6843 (2.38%), Test Loss: 21.7766 (2.31%), Recon Train: 22.6843 (2.38%), Recon Test: 21.7766 (2.31%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 19/200 [TPE: 0:00:24, RT: 1:12:34]; Train Loss: 22.6843 (2.38%), Test Loss: 21.7766 (2.31%), Recon Train: 22.6843 (2.38%), Recon Test: 21.7766 (2.31%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 20/200 [TPE: 0:00:24, RT: 1:12:10]; Train Loss: 22.2000 (2.14%), Test Loss: 21.3735 (1.85%), Recon Train: 22.2000 (2.14%), Recon Test: 21.3735 (1.85%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 20/200 [TPE: 0:00:24, RT: 1:12:10]; Train Loss: 22.2000 (2.14%), Test Loss: 21.3735 (1.85%), Recon Train: 22.2000 (2.14%), Recon Test: 21.3735 (1.85%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 21/200 [TPE: 0:00:24, RT: 1:11:46]; Train Loss: 21.7279 (2.13%), Test Loss: 20.7708 (2.82%), Recon Train: 21.7279 (2.13%), Recon Test: 20.7708 (2.82%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 21/200 [TPE: 0:00:24, RT: 1:11:46]; Train Loss: 21.7279 (2.13%), Test Loss: 20.7708 (2.82%), Recon Train: 21.7279 (2.13%), Recon Test: 20.7708 (2.82%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 22/200 [TPE: 0:00:24, RT: 1:11:23]; Train Loss: 21.2640 (2.14%), Test Loss: 20.3624 (1.97%), Recon Train: 21.2640 (2.14%), Recon Test: 20.3624 (1.97%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 22/200 [TPE: 0:00:24, RT: 1:11:23]; Train Loss: 21.2640 (2.14%), Test Loss: 20.3624 (1.97%), Recon Train: 21.2640 (2.14%), Recon Test: 20.3624 (1.97%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 23/200 [TPE: 0:00:24, RT: 1:10:59]; Train Loss: 20.8799 (1.81%), Test Loss: 19.8892 (2.32%), Recon Train: 20.8799 (1.81%), Recon Test: 19.8892 (2.32%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 23/200 [TPE: 0:00:24, RT: 1:10:59]; Train Loss: 20.8799 (1.81%), Test Loss: 19.8892 (2.32%), Recon Train: 20.8799 (1.81%), Recon Test: 19.8892 (2.32%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 24/200 [TPE: 0:00:24, RT: 1:10:35]; Train Loss: 20.5056 (1.79%), Test Loss: 19.5201 (1.86%), Recon Train: 20.5056 (1.79%), Recon Test: 19.5201 (1.86%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 24/200 [TPE: 0:00:24, RT: 1:10:35]; Train Loss: 20.5056 (1.79%), Test Loss: 19.5201 (1.86%), Recon Train: 20.5056 (1.79%), Recon Test: 19.5201 (1.86%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 25/200 [TPE: 0:00:24, RT: 1:10:12]; Train Loss: 20.1373 (1.80%), Test Loss: 19.2241 (1.52%), Recon Train: 20.1373 (1.80%), Recon Test: 19.2241 (1.52%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 25/200 [TPE: 0:00:24, RT: 1:10:12]; Train Loss: 20.1373 (1.80%), Test Loss: 19.2241 (1.52%), Recon Train: 20.1373 (1.80%), Recon Test: 19.2241 (1.52%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 26/200 [TPE: 0:00:24, RT: 1:09:48]; Train Loss: 19.8063 (1.64%), Test Loss: 18.8767 (1.81%), Recon Train: 19.8063 (1.64%), Recon Test: 18.8767 (1.81%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 26/200 [TPE: 0:00:24, RT: 1:09:48]; Train Loss: 19.8063 (1.64%), Test Loss: 18.8767 (1.81%), Recon Train: 19.8063 (1.64%), Recon Test: 18.8767 (1.81%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 27/200 [TPE: 0:00:24, RT: 1:09:26]; Train Loss: 19.5007 (1.54%), Test Loss: 18.6139 (1.39%), Recon Train: 19.5007 (1.54%), Recon Test: 18.6139 (1.39%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 27/200 [TPE: 0:00:24, RT: 1:09:26]; Train Loss: 19.5007 (1.54%), Test Loss: 18.6139 (1.39%), Recon Train: 19.5007 (1.54%), Recon Test: 18.6139 (1.39%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 28/200 [TPE: 0:00:24, RT: 1:09:03]; Train Loss: 19.2352 (1.36%), Test Loss: 18.2796 (1.80%), Recon Train: 19.2352 (1.36%), Recon Test: 18.2796 (1.80%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 28/200 [TPE: 0:00:24, RT: 1:09:03]; Train Loss: 19.2352 (1.36%), Test Loss: 18.2796 (1.80%), Recon Train: 19.2352 (1.36%), Recon Test: 18.2796 (1.80%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 29/200 [TPE: 0:00:24, RT: 1:08:39]; Train Loss: 18.9299 (1.59%), Test Loss: 18.0221 (1.41%), Recon Train: 18.9299 (1.59%), Recon Test: 18.0221 (1.41%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 29/200 [TPE: 0:00:24, RT: 1:08:39]; Train Loss: 18.9299 (1.59%), Test Loss: 18.0221 (1.41%), Recon Train: 18.9299 (1.59%), Recon Test: 18.0221 (1.41%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 30/200 [TPE: 0:00:24, RT: 1:08:16]; Train Loss: 18.6754 (1.34%), Test Loss: 17.7407 (1.56%), Recon Train: 18.6754 (1.34%), Recon Test: 17.7407 (1.56%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 30/200 [TPE: 0:00:24, RT: 1:08:16]; Train Loss: 18.6754 (1.34%), Test Loss: 17.7407 (1.56%), Recon Train: 18.6754 (1.34%), Recon Test: 17.7407 (1.56%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 31/200 [TPE: 0:00:24, RT: 1:07:53]; Train Loss: 18.4687 (1.11%), Test Loss: 17.4604 (1.58%), Recon Train: 18.4687 (1.11%), Recon Test: 17.4604 (1.58%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 31/200 [TPE: 0:00:24, RT: 1:07:53]; Train Loss: 18.4687 (1.11%), Test Loss: 17.4604 (1.58%), Recon Train: 18.4687 (1.11%), Recon Test: 17.4604 (1.58%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 32/200 [TPE: 0:00:24, RT: 1:07:30]; Train Loss: 18.2579 (1.14%), Test Loss: 17.1976 (1.51%), Recon Train: 18.2579 (1.14%), Recon Test: 17.1976 (1.51%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 32/200 [TPE: 0:00:24, RT: 1:07:30]; Train Loss: 18.2579 (1.14%), Test Loss: 17.1976 (1.51%), Recon Train: 18.2579 (1.14%), Recon Test: 17.1976 (1.51%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 33/200 [TPE: 0:00:25, RT: 1:07:10]; Train Loss: 18.0242 (1.28%), Test Loss: 17.0523 (0.85%), Recon Train: 18.0242 (1.28%), Recon Test: 17.0523 (0.85%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 33/200 [TPE: 0:00:25, RT: 1:07:10]; Train Loss: 18.0242 (1.28%), Test Loss: 17.0523 (0.85%), Recon Train: 18.0242 (1.28%), Recon Test: 17.0523 (0.85%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 34/200 [TPE: 0:00:24, RT: 1:06:50]; Train Loss: 17.8568 (0.93%), Test Loss: 16.7805 (1.59%), Recon Train: 17.8568 (0.93%), Recon Test: 16.7805 (1.59%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 34/200 [TPE: 0:00:24, RT: 1:06:50]; Train Loss: 17.8568 (0.93%), Test Loss: 16.7805 (1.59%), Recon Train: 17.8568 (0.93%), Recon Test: 16.7805 (1.59%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 35/200 [TPE: 0:00:24, RT: 1:06:28]; Train Loss: 17.6820 (0.98%), Test Loss: 16.6496 (0.78%), Recon Train: 17.6820 (0.98%), Recon Test: 16.6496 (0.78%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 35/200 [TPE: 0:00:24, RT: 1:06:28]; Train Loss: 17.6820 (0.98%), Test Loss: 16.6496 (0.78%), Recon Train: 17.6820 (0.98%), Recon Test: 16.6496 (0.78%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 36/200 [TPE: 0:00:24, RT: 1:06:03]; Train Loss: 17.4814 (1.13%), Test Loss: 16.4915 (0.95%), Recon Train: 17.4814 (1.13%), Recon Test: 16.4915 (0.95%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 36/200 [TPE: 0:00:24, RT: 1:06:03]; Train Loss: 17.4814 (1.13%), Test Loss: 16.4915 (0.95%), Recon Train: 17.4814 (1.13%), Recon Test: 16.4915 (0.95%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 37/200 [TPE: 0:00:24, RT: 1:05:39]; Train Loss: 17.3255 (0.89%), Test Loss: 16.2495 (1.47%), Recon Train: 17.3255 (0.89%), Recon Test: 16.2495 (1.47%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 37/200 [TPE: 0:00:24, RT: 1:05:39]; Train Loss: 17.3255 (0.89%), Test Loss: 16.2495 (1.47%), Recon Train: 17.3255 (0.89%), Recon Test: 16.2495 (1.47%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 38/200 [TPE: 0:00:24, RT: 1:05:16]; Train Loss: 17.1861 (0.80%), Test Loss: 16.0846 (1.01%), Recon Train: 17.1861 (0.80%), Recon Test: 16.0846 (1.01%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 38/200 [TPE: 0:00:24, RT: 1:05:16]; Train Loss: 17.1861 (0.80%), Test Loss: 16.0846 (1.01%), Recon Train: 17.1861 (0.80%), Recon Test: 16.0846 (1.01%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 39/200 [TPE: 0:00:24, RT: 1:04:52]; Train Loss: 17.0058 (1.05%), Test Loss: 15.9515 (0.83%), Recon Train: 17.0058 (1.05%), Recon Test: 15.9515 (0.83%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 39/200 [TPE: 0:00:24, RT: 1:04:52]; Train Loss: 17.0058 (1.05%), Test Loss: 15.9515 (0.83%), Recon Train: 17.0058 (1.05%), Recon Test: 15.9515 (0.83%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "def train_model(train_loader, test_loader, decoder_model, optimizers, \n",
    "                          test_rep, n_epochs, first_epoch_gmm=1, \n",
    "                          refit_gmm_interval=None, lambda_gmm=1.0, \n",
    "                          metrics_list=None, device='cuda', plot_interval=10,\n",
    "                          use_subset=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Training loop for the decoder-GMM model with separate train and test representation layers.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    use_subset: If True, only shows that subset mode is enabled (data subset is handled by IndexedDataset)\n",
    "    verbose: If True, shows detailed training output with all loss components and timing\n",
    "    \"\"\"\n",
    "    model_optimizer, rep_optimizer, testrep_optimizer = optimizers\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    gmm_train_losses = []\n",
    "    gmm_test_losses = []\n",
    "    recon_train_losses = []\n",
    "    recon_test_losses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_times = []  # Track epoch times for better RT calculation\n",
    "    \n",
    "    # Get references from the decoder model\n",
    "    model = decoder_model.decoder\n",
    "    rep = decoder_model.rep_layer\n",
    "    gmm = decoder_model.gmm\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"=== Training Configuration ===\")\n",
    "        print(f\"Epochs: {n_epochs}\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"First GMM epoch: {first_epoch_gmm}\")\n",
    "        print(f\"Plot interval: {plot_interval}\")\n",
    "        print(f\"Using subset: {use_subset}\")\n",
    "        print(f\"Verbose mode: {verbose}\")\n",
    "        print(\"=\" * 30)\n",
    "    \n",
    "    def calc_improvement(loss_list):\n",
    "        # Returns the percentage improvement compared to the previous epoch\n",
    "        if len(loss_list) < 2:\n",
    "            return 0.0\n",
    "        previous = loss_list[-2]\n",
    "        current = loss_list[-1]\n",
    "        return ((previous - current) / previous) * 100 if previous != 0 else 0.0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train_losses.append(0)\n",
    "        test_losses.append(0)\n",
    "        gmm_train_losses.append(0)\n",
    "        gmm_test_losses.append(0)\n",
    "        recon_train_losses.append(0)\n",
    "        recon_test_losses.append(0)\n",
    "\n",
    "        # Initialize or refit GMM\n",
    "        if epoch == 1 and not (first_epoch_gmm == 1):\n",
    "            if verbose:\n",
    "                print(f\"Initializing GMM at epoch {epoch}...\")\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm.fit(representations, max_iter=1)\n",
    "        elif (epoch == first_epoch_gmm) or (refit_gmm_interval and epoch % refit_gmm_interval == 0):\n",
    "            if verbose:\n",
    "                print(f\"Fitting GMM at epoch {epoch}...\")\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm = GaussianMixture(\n",
    "                    n_features=rep.z.shape[1], \n",
    "                    n_components=n_components, \n",
    "                    covariance_type=covariance_type, \n",
    "                    init_params=init_params,\n",
    "                    device=device, \n",
    "                    random_state=RANDOM_STATE, \n",
    "                    verbose=verbose and (epoch == first_epoch_gmm), \n",
    "                    max_iter=max_iter,\n",
    "                    tol=tol,\n",
    "                    n_init=n_init,\n",
    "                    warm_start=True\n",
    "                )\n",
    "                gmm.fit(representations, max_iter=1000)\n",
    "        elif epoch > first_epoch_gmm:\n",
    "            if verbose and refit_gmm_interval and epoch % refit_gmm_interval == 0:\n",
    "                print(f\"Updating GMM at epoch {epoch}...\")\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm.fit(representations, max_iter=100, warm_start=True)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        rep_optimizer.zero_grad()\n",
    "        \n",
    "        for i, (index, x, labels_batch) in enumerate(train_loader):\n",
    "            model_optimizer.zero_grad()\n",
    "            \n",
    "            x, index = x.to(device), index.to(device)\n",
    "\n",
    "            z = rep(index)\n",
    "            y = model(z)\n",
    "            recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "            \n",
    "            # Calculate loss\n",
    "            if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss + gmm_error\n",
    "            else:\n",
    "                loss = recon_loss\n",
    "                gmm_error = torch.tensor(0.0).to(device)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            train_losses[-1] += loss.item()\n",
    "            recon_train_losses[-1] += recon_loss.item()\n",
    "            if epoch >= first_epoch_gmm:\n",
    "                gmm_train_losses[-1] += gmm_error.item()\n",
    "        \n",
    "        rep_optimizer.step()\n",
    "\n",
    "        # Testing loop\n",
    "        model.eval()\n",
    "        testrep_optimizer.zero_grad()  # Zero out test rep gradients\n",
    "        \n",
    "        for i, (index, x, _) in enumerate(test_loader):\n",
    "            x, index = x.to(device), index.to(device)\n",
    "\n",
    "            z = test_rep(index)  # Use test_rep\n",
    "            y = model(z)\n",
    "            recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "            \n",
    "            # Calculate loss\n",
    "            if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss + gmm_error\n",
    "            else:\n",
    "                loss = recon_loss\n",
    "                gmm_error = torch.tensor(0.0).to(device)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # Track losses\n",
    "            test_losses[-1] += loss.item()\n",
    "            recon_test_losses[-1] += recon_loss.item()\n",
    "            if epoch >= first_epoch_gmm:\n",
    "                gmm_test_losses[-1] += gmm_error.item()\n",
    "        \n",
    "        testrep_optimizer.step()  # Update test_rep parameters\n",
    "        \n",
    "        # Normalize losses by dataset size\n",
    "        train_losses[-1] /= len(train_loader.dataset)\n",
    "        test_losses[-1] /= len(test_loader.dataset)\n",
    "        recon_train_losses[-1] /= len(train_loader.dataset)\n",
    "        recon_test_losses[-1] /= len(test_loader.dataset)\n",
    "        gmm_train_losses[-1] /= len(train_loader.dataset)\n",
    "        gmm_test_losses[-1] /= len(test_loader.dataset)\n",
    "        \n",
    "        # Calculate timing information\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_duration)\n",
    "        \n",
    "        # Calculate estimated time remaining\n",
    "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_epochs = n_epochs - epoch\n",
    "        estimated_time_remaining = remaining_epochs * avg_epoch_time\n",
    "        \n",
    "        # Format the time for display\n",
    "        epoch_time_str = str(timedelta(seconds=int(epoch_duration)))\n",
    "        remaining_time_str = str(timedelta(seconds=int(estimated_time_remaining)))\n",
    "        \n",
    "        # Print epoch statistics with configurable verbosity\n",
    "        if verbose:\n",
    "            # Detailed output with all components\n",
    "            print(f\"Epoch {epoch}/{n_epochs} \"\n",
    "                  f\"[TPE: {epoch_time_str}, RT: {remaining_time_str}]; \"\n",
    "                  f\"Train Loss: {train_losses[-1]:.4f} ({calc_improvement(train_losses):.2f}%), \"\n",
    "                  f\"Test Loss: {test_losses[-1]:.4f} ({calc_improvement(test_losses):.2f}%), \"\n",
    "                  f\"Recon Train: {recon_train_losses[-1]:.4f} ({calc_improvement(recon_train_losses):.2f}%), \"\n",
    "                  f\"Recon Test: {recon_test_losses[-1]:.4f} ({calc_improvement(recon_test_losses):.2f}%), \"\n",
    "                  f\"GMM Train: {gmm_train_losses[-1]:.4f} ({calc_improvement(gmm_train_losses):.2f}%), \"\n",
    "                  f\"GMM Test: {gmm_test_losses[-1]:.4f} ({calc_improvement(gmm_test_losses):.2f}%)\")\n",
    "        else:\n",
    "            # Compact output\n",
    "            if epoch % 10 == 0 or epoch == n_epochs:\n",
    "                print(f\"Epoch {epoch}/{n_epochs}: \"\n",
    "                      f\"Train={train_losses[-1]:.4f}, Test={test_losses[-1]:.4f}, \"\n",
    "                      f\"Time={epoch_time_str}\")\n",
    "        \n",
    "        # Plot losses at regular intervals (only if verbose or at key epochs)\n",
    "        if (verbose and (epoch % plot_interval == 0 or epoch == n_epochs)) or \\\n",
    "           (not verbose and epoch == n_epochs):\n",
    "            plot_training_losses(\n",
    "                train_losses, test_losses,\n",
    "                recon_train_losses, recon_test_losses,\n",
    "                gmm_train_losses, gmm_test_losses,\n",
    "                title=f\"Training Losses at Epoch {epoch}\"\n",
    "            )\n",
    "\n",
    "        # Plot images and visualizations (reduced frequency for non-verbose)\n",
    "        plot_this_epoch = (verbose and (epoch % plot_interval == 0 or epoch == n_epochs)) or \\\n",
    "                         (not verbose and (epoch % (plot_interval * 2) == 0 or epoch == n_epochs))\n",
    "        \n",
    "        if plot_this_epoch:\n",
    "            with torch.no_grad():\n",
    "                # Plot reconstructed training images\n",
    "                z_train = rep(indices_train.to(device))\n",
    "                reconstructions_train = model(z_train)\n",
    "                reconstructions_train = reconstructions_train.cpu()\n",
    "                plot_images(reconstructions_train, labels_train.cpu(), \"Reconstructed Train Images\", epoch=epoch)\n",
    "                \n",
    "                # Plot reconstructed test images\n",
    "                z_test = test_rep(indices_test.to(device))\n",
    "                reconstructions_test = model(z_test)\n",
    "                reconstructions_test = reconstructions_test.cpu()\n",
    "                plot_images(reconstructions_test, labels_test.cpu(), \"Reconstructed Test Images\", epoch=epoch)\n",
    "\n",
    "                # Plot GMM component reconstructions (only after GMM is fitted)\n",
    "                if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                    plot_gmm_images(\n",
    "                        model, gmm, \"GMM Component Means (by weight)\",\n",
    "                        epoch=epoch, top_n=n_components, device=device\n",
    "                    )\n",
    "                    plot_gmm_samples(\n",
    "                        model, gmm, \"Generated Images from GMM Samples\",\n",
    "                        n_samples=n_components, epoch=epoch, device=device\n",
    "                    )\n",
    "\n",
    "        # Plot latent space visualizations (reduced frequency for weak GPUs)\n",
    "        if gmm is not None and verbose:\n",
    "            if epoch % plot_interval == 0 or epoch == n_epochs or epoch == 1 or epoch == first_epoch_gmm:\n",
    "                with torch.no_grad():\n",
    "                    z_train = rep.z.detach()\n",
    "                    z_test = test_rep.z.detach()\n",
    "                    labels_train = torch.tensor([label for _, _, label in indexed_train_dataset])\n",
    "                    labels_test = torch.tensor([label for _, _, label in indexed_test_dataset])\n",
    "\n",
    "                    label_names = [\n",
    "                        \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                        \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "                    ]\n",
    "\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='pca', \n",
    "                                    title=\"Latent Space - PCA\",\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='tsne', \n",
    "                                    perplexity=30,\n",
    "                                    n_iter=1000,\n",
    "                                    title=\"Latent Space - t-SNE\",\n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='umap',\n",
    "                                    n_neighbors=20,\n",
    "                                    n_components=2,\n",
    "                                    min_dist=0.01,\n",
    "                                    title=\"Latent Space - UMAP\",\n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "        elif gmm is not None and not verbose:\n",
    "            # Only show final latent space for non-verbose mode\n",
    "            if epoch == n_epochs:\n",
    "                with torch.no_grad():\n",
    "                    z_train = rep.z.detach()\n",
    "                    z_test = test_rep.z.detach()\n",
    "                    labels_train = torch.tensor([label for _, _, label in indexed_train_dataset])\n",
    "                    labels_test = torch.tensor([label for _, _, label in indexed_test_dataset])\n",
    "\n",
    "                    label_names = [\n",
    "                        \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                        \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "                    ]\n",
    "\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='pca', \n",
    "                                    title=\"Final Latent Space - PCA\",\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "        \n",
    "    if verbose:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {str(timedelta(seconds=int(total_time)))}\")\n",
    "        print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"Final test loss: {test_losses[-1]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Training completed: {train_losses[-1]:.4f} train, {test_losses[-1]:.4f} test\")\n",
    "        \n",
    "    return decoder_model, rep, test_rep\n",
    "\n",
    "# Initialize components\n",
    "nsample_train = len(indexed_train_dataset)\n",
    "nsample_test = len(indexed_test_dataset)\n",
    "\n",
    "experiment_name = f\"experiment\"\n",
    "\n",
    "# Override the save_figure function to use experiment folder\n",
    "import src.visualization.visualise as viz_module\n",
    "\n",
    "# Store the original function ONLY ONCE to avoid recursion\n",
    "if not hasattr(viz_module, '_original_save_figure'):\n",
    "    viz_module._original_save_figure = viz_module.save_figure\n",
    "\n",
    "def save_figure_with_experiment(fig, filename, base_dir=f\"figures/{experiment_name}\", \n",
    "                               subdir=None, dpi=200, format=\"png\", close_fig=True):\n",
    "    \"\"\"Modified save_figure that uses experiment folder as base directory.\"\"\"\n",
    "    # Use the stored original function to avoid recursion\n",
    "    return viz_module._original_save_figure(fig, filename, base_dir, subdir, dpi, format, close_fig)\n",
    "\n",
    "# Replace the save_figure function globally  \n",
    "viz_module.save_figure = save_figure_with_experiment\n",
    "# =============================================================================\n",
    "\n",
    "# GMM Parameters\n",
    "n_features = 5\n",
    "n_components = 20\n",
    "covariance_type = 'diag'\n",
    "init_params = 'kmeans'\n",
    "max_iter = 1000\n",
    "tol = 1e-4\n",
    "warm_start = True\n",
    "verbose = False\n",
    "verbose_interval = 10\n",
    "n_init = 1\n",
    "\n",
    "# Representation Layer Parameters\n",
    "dist = 'uniform_ball'\n",
    "dist_params_train = {\n",
    "    \"radius\": 0.1,\n",
    "}\n",
    "dist_params_test = {\n",
    "    \"radius\": 0.1,\n",
    "}\n",
    "\n",
    "# Decoder Parameters\n",
    "decoder_hidden_dims = [128, 64, 32]\n",
    "decoder_output_channels = 1\n",
    "decoder_output_size = (28, 28)\n",
    "decoder_activation = 'leaky_relu'\n",
    "decoder_final_activation = 'sigmoid'\n",
    "decoder_use_batch_norm = True\n",
    "decoder_dropout_rate = 0.1\n",
    "decoder_init_size = (7, 7)\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 200\n",
    "first_epoch_gmm = 50\n",
    "refit_gmm_interval = 100\n",
    "lambda_gmm = 1.0\n",
    "\n",
    "# Representation Layer Optimizer\n",
    "rep_lr = 0.01\n",
    "\n",
    "# Decoder Optimizer\n",
    "decoder_lr = 0.001\n",
    "\n",
    "# Plotting Parameters\n",
    "plot_interval = 50\n",
    "\n",
    "# Create the representation layer\n",
    "rep = RepresentationLayer(dim=n_features, n_samples=nsample_train, dist=dist, dist_params=dist_params_train, device=device)\n",
    "test_rep = RepresentationLayer(dim=n_features, n_samples=nsample_test, dist=dist, dist_params=dist_params_test, device=device)\n",
    "\n",
    "# Create GMM\n",
    "gmm = GaussianMixture(\n",
    "    n_features=n_features, \n",
    "    n_components=n_components, \n",
    "    covariance_type=covariance_type, \n",
    "    init_params=init_params,\n",
    "    device=device, \n",
    "    random_state=RANDOM_STATE, \n",
    "    verbose=False, \n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    n_init=n_init,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "# Create the decoder\n",
    "decoder = ConvDecoder(\n",
    "    latent_dim=n_features,\n",
    "    hidden_dims=decoder_hidden_dims,\n",
    "    output_channels=decoder_output_channels,\n",
    "    output_size=decoder_output_size,\n",
    "    use_batch_norm=decoder_use_batch_norm,\n",
    "    activation=decoder_activation,\n",
    "    final_activation=decoder_final_activation,\n",
    "    dropout_rate=decoder_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# Create the full model\n",
    "decoder_model = DGD(decoder, rep, gmm)\n",
    "\n",
    "# Count and print parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print parameter counts\n",
    "decoder_params = count_parameters(decoder)\n",
    "rep_params = count_parameters(rep)\n",
    "test_rep_params = count_parameters(test_rep)\n",
    "\n",
    "print(f\"Decoder parameters: {decoder_params:,} ({decoder_params / 1e6:.2f}M)\")\n",
    "print(f\"Train representation parameters: {rep_params:,} ({rep_params / 1e6:.2f}M)\")\n",
    "print(f\"Test representation parameters: {test_rep_params:,} ({test_rep_params / 1e6:.2f}M)\")\n",
    "print(f\"Total trainable parameters: {decoder_params + rep_params + test_rep_params:,} ({(decoder_params + rep_params + test_rep_params) / 1e6:.2f}M)\")\n",
    "\n",
    "# Analyze parameter distribution\n",
    "print(\"\\nParameter distribution:\")\n",
    "print(f\"Decoder: {decoder_params / (decoder_params + rep_params) * 100:.1f}%\")\n",
    "print(f\"Train rep: {rep_params / (decoder_params + rep_params) * 100:.1f}%\\n\")\n",
    "\n",
    "# Initialize optimizers\n",
    "decoder_optimizer = torch.optim.AdamW(\n",
    "    decoder.parameters(),\n",
    "    lr=decoder_lr,\n",
    ")\n",
    "trainrep_optimizer = torch.optim.AdamW(\n",
    "    rep.parameters(),\n",
    "    lr=rep_lr,\n",
    ")\n",
    "testrep_optimizer = torch.optim.AdamW(\n",
    "    test_rep.parameters(),\n",
    "    lr=rep_lr,\n",
    ")\n",
    "\n",
    "optimizers = [decoder_optimizer, trainrep_optimizer, testrep_optimizer]\n",
    "\n",
    "visualizer = LatentSpaceVisualizer()\n",
    "\n",
    "# Training configuration for weak GPU support\n",
    "use_subset_mode = use_small_dataset  # Use the existing variable\n",
    "verbose_mode = True  # Set to False for compact output\n",
    "\n",
    "trained_model, trained_rep, trained_test_rep = train_model(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    decoder_model,\n",
    "    optimizers,\n",
    "    test_rep=test_rep,\n",
    "    n_epochs=epochs,\n",
    "    first_epoch_gmm=first_epoch_gmm,\n",
    "    refit_gmm_interval=refit_gmm_interval,\n",
    "    lambda_gmm=lambda_gmm,\n",
    "    device=device,\n",
    "    plot_interval=plot_interval,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
