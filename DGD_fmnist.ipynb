{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA version: 12.4\n",
      "PyTorch version: 2.5.1\n",
      "Number of CUDA devices: 2\n",
      "Current CUDA device: 0\n",
      "Allocated memory: 26.3896484375 MB\n",
      "Reserved memory: 1178.0 MB\n",
      "Total memory: 24165.25 MB\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from datetime import timedelta\n",
    "#import torch_optimizer as optim\n",
    "\n",
    "# only import if cuml is installed\n",
    "'''\n",
    "try:\n",
    "    from cuml.accel import install\n",
    "    install()\n",
    "except ImportError:\n",
    "    print(\"Cuml not installed, using CPU for umap and sklearn\")\n",
    "'''\n",
    "\n",
    "from utils import RepresentationLayer, DGD, ConvDecoder, LatentSpaceVisualizer, GaussianMixture, GMMInitializer, plot_training_losses, plot_images, plot_gmm_images, plot_gmm_samples\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated(device)/1024**2} MB\")\n",
    "    print(f\"Reserved memory: {torch.cuda.memory_reserved(device)/1024**2} MB\")\n",
    "    print(f\"Total memory: {torch.cuda.get_device_properties(device).total_memory/1024**2} MB\")\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "use_small_dataset = False\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 60000 (total: 60000)\n",
      "Test dataset: 10000 (total: 10000)\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Image size: 784\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset, use_subset=False, subset_fraction=0.1):\n",
    "        \"\"\"\n",
    "        Wrap a dataset with indices\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset: The original dataset\n",
    "        use_subset: Whether to use only a subset of the data\n",
    "        subset_fraction: The fraction of data to use if use_subset is True\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if use_subset:\n",
    "            # Create a subset of indices (10% by default)\n",
    "            total_size = len(dataset)\n",
    "            subset_size = int(total_size * subset_fraction)\n",
    "            \n",
    "            # Create random indices ensuring we get samples from all classes\n",
    "            all_indices = list(range(total_size))\n",
    "            np.random.shuffle(all_indices)\n",
    "            self.indices = all_indices[:subset_size]\n",
    "        else:\n",
    "            # Use all indices\n",
    "            self.indices = list(range(len(dataset)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Map the index to the original dataset index\n",
    "        orig_index = self.indices[index]\n",
    "        data, target = self.dataset[orig_index]\n",
    "        return orig_index, data, target\n",
    "\n",
    "# Create datasets with the option to use only a subset\n",
    "indexed_train_dataset = IndexedDataset(train_dataset, use_subset=use_small_dataset)\n",
    "indexed_test_dataset = IndexedDataset(test_dataset, use_subset=use_small_dataset)\n",
    "\n",
    "# Adjust batch size if using a small dataset\n",
    "batch_size = 128\n",
    "if use_small_dataset:\n",
    "    # Use smaller batch size for small datasets\n",
    "    batch_size = min(batch_size, max(32, len(indexed_train_dataset) // 10))\n",
    "    print(f\"Using small dataset, adjusted batch size: {batch_size}\")\n",
    "\n",
    "train_loader = DataLoader(indexed_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(indexed_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset info\n",
    "print('Train dataset:', len(indexed_train_dataset), \n",
    "      f'({\"10% of \" if use_small_dataset else \"\"}total: {len(train_dataset)})')\n",
    "print('Test dataset:', len(indexed_test_dataset), \n",
    "      f'({\"10% of \" if use_small_dataset else \"\"}total: {len(test_dataset)})')\n",
    "\n",
    "# Print shape of an image\n",
    "print('Image shape:', train_dataset[0][0].shape)\n",
    "# Print total number of pixels in an image\n",
    "print('Image size:', train_dataset[0][0].numel())\n",
    "\n",
    "all_labels = torch.tensor([train_dataset[i][1] for i in indexed_train_dataset.indices], device=device)\n",
    "\n",
    "# Get a batch of images and labels\n",
    "batch_train = next(iter(train_loader))\n",
    "indices_train, images_train, labels_train = batch_train\n",
    "\n",
    "batch_test = next(iter(test_loader))\n",
    "indices_test, images_test, labels_test = batch_test\n",
    "\n",
    "# Plot the images with their labels\n",
    "plot_images(images_train, labels_train, 'Fashion MNIST Train samples', epoch=None, cmap='viridis')\n",
    "plot_images(images_test, labels_test, 'Fashion MNIST Test samples', epoch=None, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder parameters: 247,297 (0.25M)\n",
      "Train representation parameters: 300,000 (0.30M)\n",
      "Test representation parameters: 50,000 (0.05M)\n",
      "Total trainable parameters: 597,297 (0.60M)\n",
      "\n",
      "Parameter distribution:\n",
      "Decoder: 45.2%\n",
      "Train rep: 54.8%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/maps/projects/heads/people/kbh904/ImageDGD/utils/gmm.py:752: UserWarning: EM algorithm did not converge. Try increasing max_iter or lowering tol.\n",
      "  warnings.warn(\"EM algorithm did not converge. Try increasing max_iter or lowering tol.\", UserWarning)\n",
      "/maps/projects/heads/people/kbh904/ImageDGD/utils/gmm.py:689: UserWarning: EM algorithm did not converge. Try increasing max_iter or lowering tol.\n",
      "  warnings.warn(\"EM algorithm did not converge. Try increasing max_iter or lowering tol.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 [0:00:14] - ETA: 2:01:47: Train Loss: 69.1046, Test Loss: 67.9951, Recon Train: 69.1046, Recon Test: 67.9951, GMM Train: 0.0000, GMM Test: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500 [0:00:10] - ETA: 1:42:34: Train Loss: 68.4780, Test Loss: 68.1416, Recon Train: 68.4780, Recon Test: 68.1416, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 3/500 [0:00:10] - ETA: 1:37:47: Train Loss: 68.3961, Test Loss: 67.9672, Recon Train: 68.3961, Recon Test: 67.9672, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 4/500 [0:00:10] - ETA: 1:34:10: Train Loss: 68.3273, Test Loss: 67.9481, Recon Train: 68.3273, Recon Test: 67.9481, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 5/500 [0:00:11] - ETA: 1:33:55: Train Loss: 68.2053, Test Loss: 67.8238, Recon Train: 68.2053, Recon Test: 67.8238, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 6/500 [0:00:11] - ETA: 1:33:46: Train Loss: 68.0170, Test Loss: 67.5472, Recon Train: 68.0170, Recon Test: 67.5472, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 7/500 [0:00:11] - ETA: 1:33:27: Train Loss: 67.7111, Test Loss: 67.2093, Recon Train: 67.7111, Recon Test: 67.2093, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 8/500 [0:00:11] - ETA: 1:33:51: Train Loss: 67.2855, Test Loss: 66.7838, Recon Train: 67.2855, Recon Test: 66.7838, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 9/500 [0:00:11] - ETA: 1:33:52: Train Loss: 66.7304, Test Loss: 66.1217, Recon Train: 66.7304, Recon Test: 66.1217, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 10/500 [0:00:11] - ETA: 1:33:56: Train Loss: 66.0278, Test Loss: 65.3425, Recon Train: 66.0278, Recon Test: 65.3425, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 11/500 [0:00:11] - ETA: 1:33:35: Train Loss: 65.1855, Test Loss: 64.1324, Recon Train: 65.1855, Recon Test: 64.1324, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 12/500 [0:00:11] - ETA: 1:33:24: Train Loss: 64.1896, Test Loss: 63.1359, Recon Train: 64.1896, Recon Test: 63.1359, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 13/500 [0:00:11] - ETA: 1:32:55: Train Loss: 63.1430, Test Loss: 62.0248, Recon Train: 63.1430, Recon Test: 62.0248, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 14/500 [0:00:11] - ETA: 1:32:31: Train Loss: 61.9935, Test Loss: 60.7163, Recon Train: 61.9935, Recon Test: 60.7163, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 15/500 [0:00:11] - ETA: 1:32:20: Train Loss: 60.8150, Test Loss: 59.2768, Recon Train: 60.8150, Recon Test: 59.2768, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 16/500 [0:00:11] - ETA: 1:32:01: Train Loss: 59.5297, Test Loss: 57.7887, Recon Train: 59.5297, Recon Test: 57.7887, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 17/500 [0:00:11] - ETA: 1:31:59: Train Loss: 58.1592, Test Loss: 56.4858, Recon Train: 58.1592, Recon Test: 56.4858, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 18/500 [0:00:11] - ETA: 1:31:41: Train Loss: 56.7750, Test Loss: 54.8726, Recon Train: 56.7750, Recon Test: 54.8726, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 19/500 [0:00:11] - ETA: 1:31:24: Train Loss: 55.2813, Test Loss: 53.2231, Recon Train: 55.2813, Recon Test: 53.2231, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 20/500 [0:00:11] - ETA: 1:31:09: Train Loss: 53.8060, Test Loss: 51.7377, Recon Train: 53.8060, Recon Test: 51.7377, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 21/500 [0:00:11] - ETA: 1:30:53: Train Loss: 52.3332, Test Loss: 50.4402, Recon Train: 52.3332, Recon Test: 50.4402, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 22/500 [0:00:10] - ETA: 1:30:20: Train Loss: 50.9529, Test Loss: 48.6738, Recon Train: 50.9529, Recon Test: 48.6738, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 23/500 [0:00:10] - ETA: 1:29:44: Train Loss: 49.6648, Test Loss: 47.5759, Recon Train: 49.6648, Recon Test: 47.5759, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 24/500 [0:00:10] - ETA: 1:29:11: Train Loss: 48.3635, Test Loss: 46.1031, Recon Train: 48.3635, Recon Test: 46.1031, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 25/500 [0:00:10] - ETA: 1:28:38: Train Loss: 47.1594, Test Loss: 44.9323, Recon Train: 47.1594, Recon Test: 44.9323, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 26/500 [0:00:10] - ETA: 1:28:05: Train Loss: 46.0537, Test Loss: 43.8455, Recon Train: 46.0537, Recon Test: 43.8455, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 27/500 [0:00:10] - ETA: 1:27:38: Train Loss: 44.9816, Test Loss: 42.7401, Recon Train: 44.9816, Recon Test: 42.7401, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 28/500 [0:00:10] - ETA: 1:27:10: Train Loss: 44.0255, Test Loss: 41.6062, Recon Train: 44.0255, Recon Test: 41.6062, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 29/500 [0:00:10] - ETA: 1:26:47: Train Loss: 43.1564, Test Loss: 40.9241, Recon Train: 43.1564, Recon Test: 40.9241, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 30/500 [0:00:10] - ETA: 1:26:21: Train Loss: 42.2880, Test Loss: 40.3714, Recon Train: 42.2880, Recon Test: 40.3714, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 31/500 [0:00:10] - ETA: 1:26:02: Train Loss: 41.4974, Test Loss: 39.1174, Recon Train: 41.4974, Recon Test: 39.1174, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 32/500 [0:00:11] - ETA: 1:25:54: Train Loss: 40.7655, Test Loss: 38.4732, Recon Train: 40.7655, Recon Test: 38.4732, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 33/500 [0:00:11] - ETA: 1:25:48: Train Loss: 40.0737, Test Loss: 37.6475, Recon Train: 40.0737, Recon Test: 37.6475, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 34/500 [0:00:11] - ETA: 1:25:44: Train Loss: 39.5060, Test Loss: 37.1029, Recon Train: 39.5060, Recon Test: 37.1029, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 35/500 [0:00:11] - ETA: 1:25:40: Train Loss: 38.8696, Test Loss: 36.4263, Recon Train: 38.8696, Recon Test: 36.4263, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 36/500 [0:00:11] - ETA: 1:25:35: Train Loss: 38.3059, Test Loss: 35.8357, Recon Train: 38.3059, Recon Test: 35.8357, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 37/500 [0:00:12] - ETA: 1:25:39: Train Loss: 37.8476, Test Loss: 35.3001, Recon Train: 37.8476, Recon Test: 35.3001, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 38/500 [0:00:11] - ETA: 1:25:33: Train Loss: 37.3128, Test Loss: 34.8510, Recon Train: 37.3128, Recon Test: 34.8510, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 39/500 [0:00:11] - ETA: 1:25:31: Train Loss: 36.8283, Test Loss: 34.9810, Recon Train: 36.8283, Recon Test: 34.9810, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 40/500 [0:00:11] - ETA: 1:25:23: Train Loss: 36.4462, Test Loss: 33.9009, Recon Train: 36.4462, Recon Test: 33.9009, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 41/500 [0:00:12] - ETA: 1:25:23: Train Loss: 36.0317, Test Loss: 33.4595, Recon Train: 36.0317, Recon Test: 33.4595, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 42/500 [0:00:11] - ETA: 1:25:18: Train Loss: 35.6290, Test Loss: 33.2282, Recon Train: 35.6290, Recon Test: 33.2282, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 43/500 [0:00:11] - ETA: 1:25:14: Train Loss: 35.2552, Test Loss: 32.9537, Recon Train: 35.2552, Recon Test: 32.9537, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 44/500 [0:00:10] - ETA: 1:24:52: Train Loss: 34.9402, Test Loss: 32.4871, Recon Train: 34.9402, Recon Test: 32.4871, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 45/500 [0:00:10] - ETA: 1:24:32: Train Loss: 34.6085, Test Loss: 32.0562, Recon Train: 34.6085, Recon Test: 32.0562, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 46/500 [0:00:10] - ETA: 1:24:13: Train Loss: 34.2429, Test Loss: 31.8296, Recon Train: 34.2429, Recon Test: 31.8296, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 47/500 [0:00:10] - ETA: 1:23:54: Train Loss: 33.9180, Test Loss: 31.4679, Recon Train: 33.9180, Recon Test: 31.4679, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 48/500 [0:00:10] - ETA: 1:23:34: Train Loss: 33.6366, Test Loss: 31.8402, Recon Train: 33.6366, Recon Test: 31.8402, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 49/500 [0:00:10] - ETA: 1:23:15: Train Loss: 33.3321, Test Loss: 30.8226, Recon Train: 33.3321, Recon Test: 30.8226, GMM Train: 0.0000, GMM Test: 0.0000\n",
      "Epoch 50/500 [0:00:10] - ETA: 1:23:00: Train Loss: 35.7184, Test Loss: 33.4749, Recon Train: 33.0688, Recon Test: 30.6896, GMM Train: 2.6497, GMM Test: 2.7852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500 [0:00:11] - ETA: 1:22:50: Train Loss: 35.2640, Test Loss: 33.1991, Recon Train: 32.7495, Recon Test: 30.5628, GMM Train: 2.5144, GMM Test: 2.6363\n",
      "Epoch 52/500 [0:00:10] - ETA: 1:22:34: Train Loss: 35.0728, Test Loss: 33.5069, Recon Train: 32.5562, Recon Test: 30.8704, GMM Train: 2.5166, GMM Test: 2.6366\n",
      "Epoch 53/500 [0:00:10] - ETA: 1:22:18: Train Loss: 34.7730, Test Loss: 32.4340, Recon Train: 32.2555, Recon Test: 29.7947, GMM Train: 2.5175, GMM Test: 2.6393\n",
      "Epoch 54/500 [0:00:10] - ETA: 1:22:03: Train Loss: 34.5402, Test Loss: 32.2780, Recon Train: 32.0229, Recon Test: 29.6359, GMM Train: 2.5173, GMM Test: 2.6421\n",
      "Epoch 55/500 [0:00:10] - ETA: 1:21:46: Train Loss: 34.3334, Test Loss: 32.1337, Recon Train: 31.8174, Recon Test: 29.4925, GMM Train: 2.5160, GMM Test: 2.6412\n",
      "Epoch 56/500 [0:00:10] - ETA: 1:21:32: Train Loss: 34.1151, Test Loss: 31.6789, Recon Train: 31.6013, Recon Test: 29.0415, GMM Train: 2.5138, GMM Test: 2.6373\n",
      "Epoch 57/500 [0:00:10] - ETA: 1:21:16: Train Loss: 33.9211, Test Loss: 31.5815, Recon Train: 31.4103, Recon Test: 28.9485, GMM Train: 2.5107, GMM Test: 2.6329\n",
      "Epoch 58/500 [0:00:10] - ETA: 1:21:00: Train Loss: 33.7174, Test Loss: 31.2870, Recon Train: 31.2106, Recon Test: 28.6572, GMM Train: 2.5069, GMM Test: 2.6298\n",
      "Epoch 59/500 [0:00:10] - ETA: 1:20:44: Train Loss: 33.5574, Test Loss: 31.1631, Recon Train: 31.0551, Recon Test: 28.5376, GMM Train: 2.5023, GMM Test: 2.6254\n",
      "Epoch 60/500 [0:00:10] - ETA: 1:20:31: Train Loss: 33.3090, Test Loss: 31.0144, Recon Train: 30.8122, Recon Test: 28.3936, GMM Train: 2.4968, GMM Test: 2.6208\n",
      "Epoch 61/500 [0:00:10] - ETA: 1:20:16: Train Loss: 33.0734, Test Loss: 30.6545, Recon Train: 30.5826, Recon Test: 28.0365, GMM Train: 2.4908, GMM Test: 2.6180\n",
      "Epoch 62/500 [0:00:10] - ETA: 1:20:01: Train Loss: 32.8808, Test Loss: 30.5756, Recon Train: 30.3966, Recon Test: 27.9620, GMM Train: 2.4842, GMM Test: 2.6136\n",
      "Epoch 63/500 [0:00:10] - ETA: 1:19:47: Train Loss: 32.6926, Test Loss: 30.1821, Recon Train: 30.2155, Recon Test: 27.5736, GMM Train: 2.4771, GMM Test: 2.6085\n",
      "Epoch 64/500 [0:00:10] - ETA: 1:19:32: Train Loss: 32.5089, Test Loss: 29.9558, Recon Train: 30.0425, Recon Test: 27.3579, GMM Train: 2.4664, GMM Test: 2.5979\n",
      "Epoch 65/500 [0:00:10] - ETA: 1:19:18: Train Loss: 32.3311, Test Loss: 30.0048, Recon Train: 29.8728, Recon Test: 27.4124, GMM Train: 2.4583, GMM Test: 2.5924\n",
      "Epoch 66/500 [0:00:10] - ETA: 1:19:05: Train Loss: 32.1602, Test Loss: 29.7381, Recon Train: 29.7103, Recon Test: 27.1496, GMM Train: 2.4499, GMM Test: 2.5885\n",
      "Epoch 67/500 [0:00:10] - ETA: 1:18:51: Train Loss: 31.9748, Test Loss: 29.5900, Recon Train: 29.5335, Recon Test: 27.0042, GMM Train: 2.4414, GMM Test: 2.5858\n",
      "Epoch 68/500 [0:00:10] - ETA: 1:18:38: Train Loss: 31.8419, Test Loss: 29.5142, Recon Train: 29.4097, Recon Test: 26.9313, GMM Train: 2.4323, GMM Test: 2.5830\n",
      "Epoch 69/500 [0:00:11] - ETA: 1:18:27: Train Loss: 31.6574, Test Loss: 29.1064, Recon Train: 29.2343, Recon Test: 26.5286, GMM Train: 2.4231, GMM Test: 2.5778\n",
      "Epoch 70/500 [0:00:11] - ETA: 1:18:23: Train Loss: 31.4578, Test Loss: 29.0608, Recon Train: 29.0444, Recon Test: 26.4893, GMM Train: 2.4135, GMM Test: 2.5714\n",
      "Epoch 71/500 [0:00:12] - ETA: 1:18:22: Train Loss: 31.3238, Test Loss: 28.9557, Recon Train: 28.9203, Recon Test: 26.3924, GMM Train: 2.4035, GMM Test: 2.5633\n",
      "Epoch 72/500 [0:00:10] - ETA: 1:18:11: Train Loss: 31.1658, Test Loss: 28.7414, Recon Train: 28.7726, Recon Test: 26.1876, GMM Train: 2.3932, GMM Test: 2.5538\n",
      "Epoch 73/500 [0:00:10] - ETA: 1:17:59: Train Loss: 31.0217, Test Loss: 28.7284, Recon Train: 28.6393, Recon Test: 26.1858, GMM Train: 2.3824, GMM Test: 2.5426\n",
      "Epoch 74/500 [0:00:11] - ETA: 1:17:50: Train Loss: 30.8755, Test Loss: 28.3768, Recon Train: 28.5090, Recon Test: 25.8459, GMM Train: 2.3665, GMM Test: 2.5310\n",
      "Epoch 75/500 [0:00:11] - ETA: 1:17:40: Train Loss: 30.6821, Test Loss: 28.3509, Recon Train: 28.3264, Recon Test: 25.8319, GMM Train: 2.3557, GMM Test: 2.5190\n",
      "Epoch 76/500 [0:00:11] - ETA: 1:17:30: Train Loss: 30.5599, Test Loss: 28.2154, Recon Train: 28.2151, Recon Test: 25.7064, GMM Train: 2.3448, GMM Test: 2.5091\n",
      "Epoch 77/500 [0:00:10] - ETA: 1:17:17: Train Loss: 30.3891, Test Loss: 27.9298, Recon Train: 28.0554, Recon Test: 25.4312, GMM Train: 2.3336, GMM Test: 2.4987\n",
      "Epoch 78/500 [0:00:10] - ETA: 1:17:03: Train Loss: 30.2804, Test Loss: 27.7981, Recon Train: 27.9582, Recon Test: 25.3103, GMM Train: 2.3222, GMM Test: 2.4878\n",
      "Epoch 79/500 [0:00:10] - ETA: 1:16:52: Train Loss: 30.1194, Test Loss: 27.7168, Recon Train: 27.8087, Recon Test: 25.2404, GMM Train: 2.3107, GMM Test: 2.4764\n",
      "Epoch 80/500 [0:00:10] - ETA: 1:16:38: Train Loss: 29.9870, Test Loss: 27.5630, Recon Train: 27.6881, Recon Test: 25.0973, GMM Train: 2.2989, GMM Test: 2.4657\n",
      "Epoch 81/500 [0:00:10] - ETA: 1:16:25: Train Loss: 29.8696, Test Loss: 27.4230, Recon Train: 27.5826, Recon Test: 24.9667, GMM Train: 2.2870, GMM Test: 2.4563\n",
      "Epoch 82/500 [0:00:10] - ETA: 1:16:12: Train Loss: 29.6945, Test Loss: 27.2963, Recon Train: 27.4197, Recon Test: 24.8487, GMM Train: 2.2748, GMM Test: 2.4476\n",
      "Epoch 83/500 [0:00:10] - ETA: 1:16:00: Train Loss: 29.5902, Test Loss: 27.1868, Recon Train: 27.3277, Recon Test: 24.7478, GMM Train: 2.2625, GMM Test: 2.4391\n",
      "Epoch 84/500 [0:00:11] - ETA: 1:15:50: Train Loss: 29.4890, Test Loss: 27.0742, Recon Train: 27.2390, Recon Test: 24.6432, GMM Train: 2.2500, GMM Test: 2.4310\n",
      "Epoch 85/500 [0:00:10] - ETA: 1:15:36: Train Loss: 29.3676, Test Loss: 26.8851, Recon Train: 27.1303, Recon Test: 24.4642, GMM Train: 2.2373, GMM Test: 2.4209\n",
      "Epoch 86/500 [0:00:10] - ETA: 1:15:22: Train Loss: 29.2108, Test Loss: 26.7824, Recon Train: 26.9864, Recon Test: 24.3745, GMM Train: 2.2244, GMM Test: 2.4079\n",
      "Epoch 87/500 [0:00:10] - ETA: 1:15:08: Train Loss: 29.1129, Test Loss: 26.8079, Recon Train: 26.9015, Recon Test: 24.4150, GMM Train: 2.2114, GMM Test: 2.3929\n",
      "Epoch 88/500 [0:00:10] - ETA: 1:14:55: Train Loss: 28.9757, Test Loss: 26.5283, Recon Train: 26.7775, Recon Test: 24.1496, GMM Train: 2.1982, GMM Test: 2.3787\n",
      "Epoch 89/500 [0:00:10] - ETA: 1:14:41: Train Loss: 28.8147, Test Loss: 26.4998, Recon Train: 26.6299, Recon Test: 24.1339, GMM Train: 2.1848, GMM Test: 2.3659\n",
      "Epoch 90/500 [0:00:10] - ETA: 1:14:28: Train Loss: 28.7343, Test Loss: 26.3225, Recon Train: 26.5630, Recon Test: 23.9680, GMM Train: 2.1712, GMM Test: 2.3545\n",
      "Epoch 91/500 [0:00:10] - ETA: 1:14:16: Train Loss: 28.6602, Test Loss: 26.0847, Recon Train: 26.5026, Recon Test: 23.7422, GMM Train: 2.1576, GMM Test: 2.3425\n",
      "Epoch 92/500 [0:00:10] - ETA: 1:14:04: Train Loss: 28.4682, Test Loss: 26.4443, Recon Train: 26.3245, Recon Test: 24.1138, GMM Train: 2.1437, GMM Test: 2.3305\n",
      "Epoch 93/500 [0:00:10] - ETA: 1:13:53: Train Loss: 28.3778, Test Loss: 26.0193, Recon Train: 26.2482, Recon Test: 23.6968, GMM Train: 2.1296, GMM Test: 2.3225\n",
      "Epoch 94/500 [0:00:10] - ETA: 1:13:41: Train Loss: 28.2595, Test Loss: 25.9275, Recon Train: 26.1442, Recon Test: 23.6122, GMM Train: 2.1153, GMM Test: 2.3153\n",
      "Epoch 95/500 [0:00:10] - ETA: 1:13:28: Train Loss: 28.1898, Test Loss: 25.7360, Recon Train: 26.0890, Recon Test: 23.4275, GMM Train: 2.1009, GMM Test: 2.3085\n",
      "Epoch 96/500 [0:00:10] - ETA: 1:13:16: Train Loss: 28.0503, Test Loss: 25.8331, Recon Train: 25.9641, Recon Test: 23.5321, GMM Train: 2.0862, GMM Test: 2.3009\n",
      "Epoch 97/500 [0:00:10] - ETA: 1:13:04: Train Loss: 27.9173, Test Loss: 25.5541, Recon Train: 25.8459, Recon Test: 23.2650, GMM Train: 2.0713, GMM Test: 2.2890\n",
      "Epoch 98/500 [0:00:10] - ETA: 1:12:51: Train Loss: 27.8551, Test Loss: 25.5448, Recon Train: 25.7989, Recon Test: 23.2677, GMM Train: 2.0562, GMM Test: 2.2771\n",
      "Epoch 99/500 [0:00:10] - ETA: 1:12:39: Train Loss: 27.7632, Test Loss: 25.4255, Recon Train: 25.7223, Recon Test: 23.1630, GMM Train: 2.0409, GMM Test: 2.2625\n",
      "Epoch 100/500 [0:00:10] - ETA: 1:12:28: Train Loss: 27.5724, Test Loss: 25.2910, Recon Train: 25.5455, Recon Test: 23.0456, GMM Train: 2.0270, GMM Test: 2.2454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500 [0:00:10] - ETA: 1:12:16: Train Loss: 27.5234, Test Loss: 25.1468, Recon Train: 25.5126, Recon Test: 22.9201, GMM Train: 2.0108, GMM Test: 2.2266\n",
      "Epoch 102/500 [0:00:10] - ETA: 1:12:04: Train Loss: 27.4118, Test Loss: 24.9935, Recon Train: 25.4173, Recon Test: 22.7848, GMM Train: 1.9945, GMM Test: 2.2087\n",
      "Epoch 103/500 [0:00:11] - ETA: 1:11:54: Train Loss: 27.2960, Test Loss: 25.0907, Recon Train: 25.3181, Recon Test: 22.8988, GMM Train: 1.9779, GMM Test: 2.1919\n",
      "Epoch 104/500 [0:00:10] - ETA: 1:11:43: Train Loss: 27.2215, Test Loss: 24.8900, Recon Train: 25.2604, Recon Test: 22.7143, GMM Train: 1.9611, GMM Test: 2.1757\n",
      "Epoch 105/500 [0:00:11] - ETA: 1:11:32: Train Loss: 27.1075, Test Loss: 24.7475, Recon Train: 25.1636, Recon Test: 22.5900, GMM Train: 1.9439, GMM Test: 2.1574\n",
      "Epoch 106/500 [0:00:10] - ETA: 1:11:20: Train Loss: 27.0643, Test Loss: 24.7663, Recon Train: 25.1380, Recon Test: 22.6266, GMM Train: 1.9264, GMM Test: 2.1397\n",
      "Epoch 107/500 [0:00:10] - ETA: 1:11:08: Train Loss: 26.9321, Test Loss: 24.5932, Recon Train: 25.0237, Recon Test: 22.4701, GMM Train: 1.9084, GMM Test: 2.1230\n",
      "Epoch 108/500 [0:00:10] - ETA: 1:10:56: Train Loss: 26.8434, Test Loss: 24.4399, Recon Train: 24.9533, Recon Test: 22.3357, GMM Train: 1.8901, GMM Test: 2.1042\n",
      "Epoch 109/500 [0:00:10] - ETA: 1:10:45: Train Loss: 26.7773, Test Loss: 24.3145, Recon Train: 24.9059, Recon Test: 22.2273, GMM Train: 1.8713, GMM Test: 2.0872\n",
      "Epoch 110/500 [0:00:10] - ETA: 1:10:33: Train Loss: 26.6416, Test Loss: 24.2370, Recon Train: 24.7895, Recon Test: 22.1672, GMM Train: 1.8521, GMM Test: 2.0697\n",
      "Epoch 111/500 [0:00:10] - ETA: 1:10:21: Train Loss: 26.5266, Test Loss: 24.2873, Recon Train: 24.6943, Recon Test: 22.2371, GMM Train: 1.8323, GMM Test: 2.0502\n",
      "Epoch 112/500 [0:00:10] - ETA: 1:10:09: Train Loss: 26.4284, Test Loss: 24.0137, Recon Train: 24.6166, Recon Test: 21.9803, GMM Train: 1.8118, GMM Test: 2.0334\n",
      "Epoch 113/500 [0:00:10] - ETA: 1:09:57: Train Loss: 26.3664, Test Loss: 23.9231, Recon Train: 24.5759, Recon Test: 21.9099, GMM Train: 1.7905, GMM Test: 2.0132\n",
      "Epoch 114/500 [0:00:10] - ETA: 1:09:45: Train Loss: 26.2799, Test Loss: 23.8685, Recon Train: 24.5116, Recon Test: 21.8748, GMM Train: 1.7683, GMM Test: 1.9937\n",
      "Epoch 115/500 [0:00:10] - ETA: 1:09:34: Train Loss: 26.1642, Test Loss: 23.8673, Recon Train: 24.4192, Recon Test: 21.8928, GMM Train: 1.7450, GMM Test: 1.9745\n",
      "Epoch 116/500 [0:00:11] - ETA: 1:09:26: Train Loss: 26.0741, Test Loss: 23.7059, Recon Train: 24.3540, Recon Test: 21.7498, GMM Train: 1.7201, GMM Test: 1.9561\n",
      "Epoch 117/500 [0:00:11] - ETA: 1:09:19: Train Loss: 26.0302, Test Loss: 23.6256, Recon Train: 24.3371, Recon Test: 21.6919, GMM Train: 1.6931, GMM Test: 1.9337\n",
      "Epoch 118/500 [0:00:11] - ETA: 1:09:11: Train Loss: 25.9018, Test Loss: 23.4567, Recon Train: 24.2390, Recon Test: 21.5509, GMM Train: 1.6629, GMM Test: 1.9058\n",
      "Epoch 119/500 [0:00:11] - ETA: 1:09:03: Train Loss: 25.7975, Test Loss: 23.4086, Recon Train: 24.1703, Recon Test: 21.5380, GMM Train: 1.6272, GMM Test: 1.8706\n",
      "Epoch 120/500 [0:00:11] - ETA: 1:08:55: Train Loss: 25.7317, Test Loss: 23.2889, Recon Train: 24.1499, Recon Test: 21.4686, GMM Train: 1.5817, GMM Test: 1.8202\n",
      "Epoch 121/500 [0:00:11] - ETA: 1:08:47: Train Loss: 25.5815, Test Loss: 23.2533, Recon Train: 24.0555, Recon Test: 21.4933, GMM Train: 1.5259, GMM Test: 1.7600\n",
      "Epoch 122/500 [0:00:12] - ETA: 1:08:40: Train Loss: 25.4933, Test Loss: 23.2051, Recon Train: 23.9943, Recon Test: 21.4702, GMM Train: 1.4991, GMM Test: 1.7349\n",
      "Epoch 123/500 [0:00:11] - ETA: 1:08:32: Train Loss: 25.4244, Test Loss: 23.2266, Recon Train: 23.9403, Recon Test: 21.5099, GMM Train: 1.4841, GMM Test: 1.7167\n",
      "Epoch 124/500 [0:00:12] - ETA: 1:08:25: Train Loss: 25.3153, Test Loss: 23.2173, Recon Train: 23.8540, Recon Test: 21.5221, GMM Train: 1.4613, GMM Test: 1.6952\n",
      "Epoch 125/500 [0:00:12] - ETA: 1:08:18: Train Loss: 25.2458, Test Loss: 22.9482, Recon Train: 23.8169, Recon Test: 21.2807, GMM Train: 1.4289, GMM Test: 1.6675\n",
      "Epoch 126/500 [0:00:11] - ETA: 1:08:10: Train Loss: 25.1123, Test Loss: 22.8594, Recon Train: 23.7187, Recon Test: 21.2185, GMM Train: 1.3937, GMM Test: 1.6409\n",
      "Epoch 127/500 [0:00:11] - ETA: 1:08:00: Train Loss: 25.0913, Test Loss: 22.7418, Recon Train: 23.7235, Recon Test: 21.1212, GMM Train: 1.3678, GMM Test: 1.6206\n",
      "Epoch 128/500 [0:00:11] - ETA: 1:07:51: Train Loss: 24.9487, Test Loss: 22.6632, Recon Train: 23.6055, Recon Test: 21.0659, GMM Train: 1.3432, GMM Test: 1.5973\n",
      "Epoch 129/500 [0:00:10] - ETA: 1:07:39: Train Loss: 24.8680, Test Loss: 22.5613, Recon Train: 23.5548, Recon Test: 20.9916, GMM Train: 1.3132, GMM Test: 1.5696\n",
      "Epoch 130/500 [0:00:10] - ETA: 1:07:27: Train Loss: 24.7910, Test Loss: 22.5219, Recon Train: 23.5129, Recon Test: 20.9866, GMM Train: 1.2781, GMM Test: 1.5353\n",
      "Epoch 131/500 [0:00:10] - ETA: 1:07:15: Train Loss: 24.7034, Test Loss: 22.3529, Recon Train: 23.4573, Recon Test: 20.8447, GMM Train: 1.2461, GMM Test: 1.5082\n",
      "Epoch 132/500 [0:00:10] - ETA: 1:07:02: Train Loss: 24.6519, Test Loss: 22.4435, Recon Train: 23.4382, Recon Test: 20.9657, GMM Train: 1.2137, GMM Test: 1.4778\n",
      "Epoch 133/500 [0:00:10] - ETA: 1:06:50: Train Loss: 24.5725, Test Loss: 22.2376, Recon Train: 23.4008, Recon Test: 20.7915, GMM Train: 1.1717, GMM Test: 1.4461\n",
      "Epoch 134/500 [0:00:10] - ETA: 1:06:38: Train Loss: 24.4525, Test Loss: 22.1855, Recon Train: 23.3189, Recon Test: 20.7691, GMM Train: 1.1336, GMM Test: 1.4164\n",
      "Epoch 135/500 [0:00:11] - ETA: 1:06:28: Train Loss: 24.3589, Test Loss: 22.0620, Recon Train: 23.2535, Recon Test: 20.6718, GMM Train: 1.1054, GMM Test: 1.3902\n",
      "Epoch 136/500 [0:00:11] - ETA: 1:06:18: Train Loss: 24.3138, Test Loss: 22.1527, Recon Train: 23.2343, Recon Test: 20.7889, GMM Train: 1.0795, GMM Test: 1.3638\n",
      "Epoch 137/500 [0:00:11] - ETA: 1:06:09: Train Loss: 24.2431, Test Loss: 21.9375, Recon Train: 23.1971, Recon Test: 20.6074, GMM Train: 1.0459, GMM Test: 1.3301\n",
      "Epoch 138/500 [0:00:11] - ETA: 1:06:01: Train Loss: 24.1170, Test Loss: 21.8455, Recon Train: 23.1060, Recon Test: 20.5465, GMM Train: 1.0110, GMM Test: 1.2990\n",
      "Epoch 139/500 [0:00:11] - ETA: 1:05:53: Train Loss: 24.0513, Test Loss: 21.8255, Recon Train: 23.0750, Recon Test: 20.5542, GMM Train: 0.9763, GMM Test: 1.2713\n",
      "Epoch 140/500 [0:00:11] - ETA: 1:05:43: Train Loss: 24.0137, Test Loss: 21.6328, Recon Train: 23.0737, Recon Test: 20.3936, GMM Train: 0.9400, GMM Test: 1.2392\n",
      "Epoch 141/500 [0:00:11] - ETA: 1:05:34: Train Loss: 23.8788, Test Loss: 21.6187, Recon Train: 22.9719, Recon Test: 20.4020, GMM Train: 0.9068, GMM Test: 1.2167\n",
      "Epoch 142/500 [0:00:11] - ETA: 1:05:25: Train Loss: 23.8238, Test Loss: 21.5373, Recon Train: 22.9598, Recon Test: 20.3544, GMM Train: 0.8640, GMM Test: 1.1829\n",
      "Epoch 143/500 [0:00:12] - ETA: 1:05:17: Train Loss: 23.7158, Test Loss: 21.4509, Recon Train: 22.8961, Recon Test: 20.3019, GMM Train: 0.8197, GMM Test: 1.1490\n",
      "Epoch 144/500 [0:00:11] - ETA: 1:05:08: Train Loss: 23.6834, Test Loss: 21.3612, Recon Train: 22.9209, Recon Test: 20.2586, GMM Train: 0.7625, GMM Test: 1.1026\n",
      "Epoch 145/500 [0:00:11] - ETA: 1:04:58: Train Loss: 23.5474, Test Loss: 21.5923, Recon Train: 22.8402, Recon Test: 20.5442, GMM Train: 0.7072, GMM Test: 1.0481\n",
      "Epoch 146/500 [0:00:11] - ETA: 1:04:49: Train Loss: 23.4761, Test Loss: 21.1969, Recon Train: 22.8063, Recon Test: 20.1760, GMM Train: 0.6698, GMM Test: 1.0208\n",
      "Epoch 147/500 [0:00:11] - ETA: 1:04:40: Train Loss: 23.3909, Test Loss: 21.2315, Recon Train: 22.7311, Recon Test: 20.2281, GMM Train: 0.6598, GMM Test: 1.0034\n",
      "Epoch 148/500 [0:00:11] - ETA: 1:04:29: Train Loss: 23.3334, Test Loss: 21.1545, Recon Train: 22.7160, Recon Test: 20.2042, GMM Train: 0.6174, GMM Test: 0.9503\n",
      "Epoch 149/500 [0:00:10] - ETA: 1:04:17: Train Loss: 23.2726, Test Loss: 20.9541, Recon Train: 22.6711, Recon Test: 20.0197, GMM Train: 0.6014, GMM Test: 0.9344\n",
      "Epoch 150/500 [0:00:10] - ETA: 1:04:05: Train Loss: 23.1499, Test Loss: 20.9437, Recon Train: 22.5953, Recon Test: 20.0638, GMM Train: 0.5547, GMM Test: 0.8798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/kbh904/.conda/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/500 [0:00:10] - ETA: 1:03:53: Train Loss: 23.1121, Test Loss: 20.7839, Recon Train: 22.5634, Recon Test: 19.9134, GMM Train: 0.5487, GMM Test: 0.8704\n",
      "Epoch 152/500 [0:00:10] - ETA: 1:03:41: Train Loss: 23.0419, Test Loss: 20.8085, Recon Train: 22.5347, Recon Test: 19.9797, GMM Train: 0.5071, GMM Test: 0.8288\n",
      "Epoch 153/500 [0:00:10] - ETA: 1:03:29: Train Loss: 22.9505, Test Loss: 20.6818, Recon Train: 22.4735, Recon Test: 19.8809, GMM Train: 0.4770, GMM Test: 0.8008\n",
      "Epoch 154/500 [0:00:10] - ETA: 1:03:16: Train Loss: 22.8861, Test Loss: 20.5589, Recon Train: 22.4419, Recon Test: 19.7885, GMM Train: 0.4443, GMM Test: 0.7705\n",
      "Epoch 155/500 [0:00:10] - ETA: 1:03:04: Train Loss: 22.8158, Test Loss: 20.4993, Recon Train: 22.4051, Recon Test: 19.7566, GMM Train: 0.4108, GMM Test: 0.7427\n",
      "Epoch 156/500 [0:00:10] - ETA: 1:02:52: Train Loss: 22.7861, Test Loss: 20.4410, Recon Train: 22.4036, Recon Test: 19.7254, GMM Train: 0.3825, GMM Test: 0.7156\n",
      "Epoch 157/500 [0:00:10] - ETA: 1:02:40: Train Loss: 22.6797, Test Loss: 20.4073, Recon Train: 22.3403, Recon Test: 19.7373, GMM Train: 0.3394, GMM Test: 0.6700\n",
      "Epoch 158/500 [0:00:10] - ETA: 1:02:28: Train Loss: 22.5817, Test Loss: 20.4689, Recon Train: 22.2684, Recon Test: 19.8240, GMM Train: 0.3134, GMM Test: 0.6449\n",
      "Epoch 159/500 [0:00:10] - ETA: 1:02:16: Train Loss: 22.5276, Test Loss: 20.1539, Recon Train: 22.2564, Recon Test: 19.5424, GMM Train: 0.2712, GMM Test: 0.6115\n",
      "Epoch 160/500 [0:00:10] - ETA: 1:02:04: Train Loss: 22.4894, Test Loss: 20.1998, Recon Train: 22.2531, Recon Test: 19.6294, GMM Train: 0.2363, GMM Test: 0.5704\n"
     ]
    }
   ],
   "source": [
    "def train_model(train_loader, test_loader, decoder_model, optimizers, \n",
    "                          test_rep, n_epochs, first_epoch_gmm=1, \n",
    "                          refit_gmm_interval=None, lambda_gmm=1.0, \n",
    "                          metrics_list=None, device='cuda', plot_interval=10):\n",
    "    \"\"\"\n",
    "    Training loop for the decoder-GMM model with separate train and test representation layers.\n",
    "    \"\"\"\n",
    "    model_optimizer, rep_optimizer, testrep_optimizer = optimizers\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    gmm_train_losses = []\n",
    "    gmm_test_losses = []\n",
    "    recon_train_losses = []\n",
    "    recon_test_losses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_times = []  # Track epoch times for better RT calculation\n",
    "    \n",
    "    # Get references from the decoder model\n",
    "    model = decoder_model.decoder\n",
    "    rep = decoder_model.rep_layer\n",
    "    gmm = decoder_model.gmm\n",
    "    \n",
    "    def calc_improvement(loss_list):\n",
    "        # Returns the percentage improvement compared to the previous epoch\n",
    "        if len(loss_list) < 2:\n",
    "            return 0.0\n",
    "        previous = loss_list[-2]\n",
    "        current = loss_list[-1]\n",
    "        return ((previous - current) / previous) * 100 if previous != 0 else 0.0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train_losses.append(0)\n",
    "        test_losses.append(0)\n",
    "        gmm_train_losses.append(0)\n",
    "        gmm_test_losses.append(0)\n",
    "        recon_train_losses.append(0)\n",
    "        recon_test_losses.append(0)\n",
    "\n",
    "        # Initialize or refit GMM\n",
    "        if epoch == 1:\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm.fit(representations, max_iter=1)\n",
    "        elif (epoch == first_epoch_gmm + 1) or (refit_gmm_interval and epoch % refit_gmm_interval == 0):\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm = GaussianMixture(\n",
    "                    n_features=rep.z.shape[1], \n",
    "                    n_components=n_components, \n",
    "                    covariance_type=covariance_type, \n",
    "                    init_params=init_params,\n",
    "                    device=device, \n",
    "                    random_state=RANDOM_STATE, \n",
    "                    verbose=verbose, \n",
    "                    max_iter=max_iter,\n",
    "                    tol=tol,\n",
    "                    n_init=n_init,\n",
    "                    warm_start=True\n",
    "                )\n",
    "                gmm.fit(representations, max_iter=1000)\n",
    "        elif epoch > first_epoch_gmm + 1:\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm.fit(representations, max_iter=100, warm_start=True)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        rep_optimizer.zero_grad()\n",
    "        \n",
    "        for i, (index, x, labels_batch) in enumerate(train_loader):\n",
    "            model_optimizer.zero_grad()\n",
    "            \n",
    "            x, index = x.to(device), index.to(device)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                # Using GMM\n",
    "                z = rep(index)\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss + gmm_error\n",
    "            else:\n",
    "                # Without GMM\n",
    "                z = rep(index)\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                loss = recon_loss\n",
    "                gmm_error = torch.tensor(0.0).to(device)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            train_losses[-1] += loss.item()\n",
    "            recon_train_losses[-1] += recon_loss.item()\n",
    "            if epoch >= first_epoch_gmm:\n",
    "                gmm_train_losses[-1] += gmm_error.item()\n",
    "        \n",
    "        rep_optimizer.step()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Testing loop\n",
    "        model.eval()\n",
    "        testrep_optimizer.zero_grad()  # Zero out test rep gradients\n",
    "        \n",
    "        for i, (index, x, _) in enumerate(test_loader):\n",
    "            x, index = x.to(device), index.to(device)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                # Using GMM\n",
    "                z = test_rep(index)  # Use test_rep\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss + gmm_error\n",
    "                \n",
    "                # Backpropagate for test_rep\n",
    "                loss.backward()\n",
    "            else:\n",
    "                # Without GMM\n",
    "                z = test_rep(index)  # Use test_rep\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                loss = recon_loss\n",
    "                gmm_error = torch.tensor(0.0).to(device)\n",
    "                \n",
    "                # Backpropagate for test_rep\n",
    "                loss.backward()\n",
    "            \n",
    "            # Track losses\n",
    "            test_losses[-1] += loss.item()\n",
    "            recon_test_losses[-1] += recon_loss.item()\n",
    "            if epoch >= first_epoch_gmm:\n",
    "                gmm_test_losses[-1] += gmm_error.item()\n",
    "        \n",
    "        testrep_optimizer.step()  # Update test_rep parameters\n",
    "        \n",
    "        # Normalize losses by dataset size\n",
    "        train_losses[-1] /= len(train_loader.dataset)\n",
    "        test_losses[-1] /= len(test_loader.dataset)\n",
    "        recon_train_losses[-1] /= len(train_loader.dataset)\n",
    "        recon_test_losses[-1] /= len(test_loader.dataset)\n",
    "        gmm_train_losses[-1] /= len(train_loader.dataset)\n",
    "        gmm_test_losses[-1] /= len(test_loader.dataset)\n",
    "        \n",
    "        # Calculate timing information\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_duration)\n",
    "        \n",
    "        # Calculate estimated time remaining\n",
    "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_epochs = n_epochs - epoch\n",
    "        estimated_time_remaining = remaining_epochs * avg_epoch_time\n",
    "        \n",
    "        # Format the time for display\n",
    "        epoch_time_str = str(timedelta(seconds=int(epoch_duration)))\n",
    "        remaining_time_str = str(timedelta(seconds=int(estimated_time_remaining)))\n",
    "        \n",
    "        # Print epoch statistics with timing information\n",
    "        print(f\"Epoch {epoch}/{n_epochs} \"\n",
    "              f\"[TPE: {epoch_time_str}, RT: {remaining_time_str}]; \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f} ({calc_improvement(train_losses):.2f}%), \"\n",
    "              f\"Test Loss: {test_losses[-1]:.4f} ({calc_improvement(test_losses):.2f}%), \"\n",
    "              f\"Recon Train: {recon_train_losses[-1]:.4f} ({calc_improvement(recon_train_losses):.2f}%), \"\n",
    "              f\"Recon Test: {recon_test_losses[-1]:.4f} ({calc_improvement(recon_test_losses):.2f}%), \"\n",
    "              f\"GMM Train: {gmm_train_losses[-1]:.4f} ({calc_improvement(gmm_train_losses):.2f}%), \"\n",
    "              f\"GMM Test: {gmm_test_losses[-1]:.4f} ({calc_improvement(gmm_test_losses):.2f}%)\")\n",
    "        \n",
    "        # Plot losses at regular intervals\n",
    "        if epoch % plot_interval == 0 or epoch == n_epochs:\n",
    "            plot_training_losses(\n",
    "                train_losses, test_losses,\n",
    "                recon_train_losses, recon_test_losses,\n",
    "                gmm_train_losses, gmm_test_losses,\n",
    "                title=f\"Training Losses at Epoch {epoch}\"\n",
    "            )\n",
    "\n",
    "        if epoch % plot_interval == 0 or epoch == n_epochs:\n",
    "            with torch.no_grad():\n",
    "                # Plot reconstructed training images\n",
    "                z_train = rep(indices_train.to(device))\n",
    "                reconstructions_train = model(z_train)\n",
    "                reconstructions_train = reconstructions_train.cpu()\n",
    "                plot_images(reconstructions_train, labels_train.cpu(), \"Reconstructed Train Images\", epoch=epoch)\n",
    "                \n",
    "                # Plot reconstructed test images\n",
    "                z_test = test_rep(indices_test.to(device))\n",
    "                reconstructions_test = model(z_test)\n",
    "                reconstructions_test = reconstructions_test.cpu()\n",
    "                plot_images(reconstructions_test, labels_test.cpu(), \"Reconstructed Test Images\", epoch=epoch)\n",
    "\n",
    "                # Plot GMM component reconstructions (only after GMM is fitted)\n",
    "                if epoch >= first_epoch_gmm + 1 and gmm is not None:\n",
    "                    plot_gmm_images(\n",
    "                        model, gmm, \"GMM Component Means (by weight)\",\n",
    "                        epoch=epoch, top_n=n_components, device=device\n",
    "                    )\n",
    "                    plot_gmm_samples(\n",
    "                        model, gmm, \"Generated Images from GMM Samples\",\n",
    "                        n_samples=n_components, epoch=epoch, device=device\n",
    "                    )\n",
    "\n",
    "        # Plot latent space visualizations\n",
    "        if gmm is not None:\n",
    "            if epoch % plot_interval == 0 or epoch == n_epochs or epoch == 1:\n",
    "                with torch.no_grad():\n",
    "                    z_train = rep.z.detach()\n",
    "                    z_test = test_rep.z.detach()\n",
    "                    labels_train = torch.tensor([label for _, _, label in indexed_train_dataset])\n",
    "                    labels_test = torch.tensor([label for _, _, label in indexed_test_dataset])\n",
    "\n",
    "                    label_names = [\n",
    "                        \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                        \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "                    ]\n",
    "\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='pca', \n",
    "                                    title=\"Latent Space - PCA\",\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "                    # visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                    #                 method='kpca', \n",
    "                    #                 kernel='rbf',\n",
    "                    #                 title=\"Latent Space - Kernel PCA\",\n",
    "                    #                 label_names=label_names,\n",
    "                    #                 epoch=epoch)\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='tsne', \n",
    "                                    perplexity=30,\n",
    "                                    n_iter=1000,\n",
    "                                    title=\"Latent Space - t-SNE\",\n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='umap',\n",
    "                                    n_neighbors=20,\n",
    "                                    n_components=2,\n",
    "                                    min_dist=0.01,\n",
    "                                    title=\"Latent Space - UMAP\",\n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "        \n",
    "    return decoder_model, rep, test_rep\n",
    "\n",
    "# Initialize components\n",
    "nsample_train = len(indexed_train_dataset)\n",
    "nsample_test = len(indexed_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GMM Parameters\n",
    "n_features = 5\n",
    "n_components = 20\n",
    "covariance_type = 'diag'\n",
    "init_params = 'kmeans'\n",
    "max_iter = 1000\n",
    "tol = 1e-4\n",
    "warm_start = True\n",
    "verbose = False\n",
    "verbose_interval = 10\n",
    "n_init = 1\n",
    "\n",
    "\n",
    "\n",
    "# Representation Layer Parameters\n",
    "dist = 'uniform_ball'\n",
    "dist_options_train = {\n",
    "    \"n_samples\": nsample_train,\n",
    "    \"dim\": n_features,\n",
    "    \"radius\": 1,\n",
    "}\n",
    "dist_options_test = {\n",
    "    \"n_samples\": nsample_test,\n",
    "    \"dim\": n_features,\n",
    "    \"radius\": 1,\n",
    "}\n",
    "\n",
    "# Decoder Parameters\n",
    "decoder_hidden_dims = [128, 64, 32]\n",
    "decoder_output_channels = 1\n",
    "decoder_output_size = (28, 28)\n",
    "decoder_activation = 'leaky_relu'\n",
    "decoder_final_activation = 'sigmoid'\n",
    "decoder_use_batch_norm = True\n",
    "decoder_dropout_rate = 0.1\n",
    "decoder_init_size = (7, 7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 200\n",
    "first_epoch_gmm = 50\n",
    "refit_gmm_interval = 100\n",
    "lambda_gmm = 1.0\n",
    "\n",
    "# Representation Layer Optimizer\n",
    "rep_lr = 0.01\n",
    "\n",
    "# Decoder Optimizer\n",
    "decoder_lr = 0.001\n",
    "\n",
    "\n",
    "# Plotting Parameters\n",
    "plot_interval = 50\n",
    "\n",
    "\n",
    "\n",
    "# Create the representation layer\n",
    "rep = RepresentationLayer(dist=dist, dist_options=dist_options_train, device=device)\n",
    "test_rep = RepresentationLayer(dist=dist, dist_options=dist_options_test, device=device)\n",
    "\n",
    "\n",
    "# Create GMM\n",
    "gmm = GaussianMixture(\n",
    "    n_features=n_features, \n",
    "    n_components=n_components, \n",
    "    covariance_type=covariance_type, \n",
    "    init_params=init_params,\n",
    "    device=device, \n",
    "    random_state=RANDOM_STATE, \n",
    "    verbose=False, \n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    n_init=n_init,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "# Create the decoder\n",
    "decoder = ConvDecoder(\n",
    "    latent_dim=n_features,\n",
    "    hidden_dims=decoder_hidden_dims,\n",
    "    output_channels=decoder_output_channels,\n",
    "    output_size=decoder_output_size,\n",
    "    use_batch_norm=decoder_use_batch_norm,\n",
    "    activation=decoder_activation,\n",
    "    final_activation=decoder_final_activation,\n",
    "    dropout_rate=decoder_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# Create the full model\n",
    "decoder_model = DGD(decoder, rep, gmm)\n",
    "\n",
    "\n",
    "# Count and print parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print parameter counts\n",
    "decoder_params = count_parameters(decoder)\n",
    "rep_params = count_parameters(rep)\n",
    "test_rep_params = count_parameters(test_rep)\n",
    "\n",
    "print(f\"Decoder parameters: {decoder_params:,} ({decoder_params / 1e6:.2f}M)\")\n",
    "print(f\"Train representation parameters: {rep_params:,} ({rep_params / 1e6:.2f}M)\")\n",
    "print(f\"Test representation parameters: {test_rep_params:,} ({test_rep_params / 1e6:.2f}M)\")\n",
    "print(f\"Total trainable parameters: {decoder_params + rep_params + test_rep_params:,} ({(decoder_params + rep_params + test_rep_params) / 1e6:.2f}M)\")\n",
    "\n",
    "# Analyze parameter distribution\n",
    "print(\"\\nParameter distribution:\")\n",
    "print(f\"Decoder: {decoder_params / (decoder_params + rep_params) * 100:.1f}%\")\n",
    "print(f\"Train rep: {rep_params / (decoder_params + rep_params) * 100:.1f}%\\n\")\n",
    "\n",
    "\n",
    "# Initialize optimizers\n",
    "decoder_optimizer = torch.optim.AdamW(\n",
    "    decoder.parameters(), \n",
    "    lr=decoder_lr, \n",
    ")\n",
    "trainrep_optimizer = torch.optim.AdamW(\n",
    "    rep.parameters(),\n",
    "    lr=rep_lr,\n",
    ")\n",
    "testrep_optimizer = torch.optim.AdamW(\n",
    "    test_rep.parameters(),\n",
    "    lr=rep_lr,\n",
    ")\n",
    "\n",
    "optimizers = [decoder_optimizer, trainrep_optimizer, testrep_optimizer]\n",
    "\n",
    "visualizer = LatentSpaceVisualizer()\n",
    "\n",
    "trained_model, trained_rep, trained_test_rep = train_model(\n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    decoder_model, \n",
    "    optimizers, \n",
    "    test_rep=test_rep, \n",
    "    n_epochs=epochs, \n",
    "    first_epoch_gmm=first_epoch_gmm, \n",
    "    refit_gmm_interval=refit_gmm_interval, \n",
    "    lambda_gmm=lambda_gmm,\n",
    "    device=device,\n",
    "    plot_interval=plot_interval\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
