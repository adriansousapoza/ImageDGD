{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuML: Installed accelerator for sklearn.\n",
      "cuML: Installed accelerator for umap.\n",
      "cuML: Successfully initialized accelerator.\n",
      "PyTorch CUDA version: 12.8\n",
      "PyTorch version: 2.7.0+cu128\n",
      "Number of CUDA devices: 2\n",
      "Device 0: NVIDIA A30\n",
      "  Memory Allocated: 0.0 MB\n",
      "  Memory Reserved: 0.0 MB\n",
      "  Total Memory: 24165.25 MB\n",
      "Device 1: NVIDIA A30\n",
      "  Memory Allocated: 26.15869140625 MB\n",
      "  Memory Reserved: 1214.0 MB\n",
      "  Total Memory: 24165.25 MB\n",
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import umap\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from datetime import timedelta\n",
    "#import torch_optimizer as optim\n",
    "\n",
    "try:\n",
    "    from cuml.accel import install\n",
    "    install()\n",
    "except ImportError:\n",
    "    print(\"Cuml not installed, using CPU for umap and sklearn\")\n",
    "\n",
    "from utils import RepresentationLayer, DGD, ConvDecoder, LatentSpaceVisualizer, GaussianMixture, GMMInitializer, plot_training_losses, plot_images, plot_gmm_images, plot_gmm_samples\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i)/1024**2} MB\")\n",
    "        print(f\"  Memory Reserved: {torch.cuda.memory_reserved(i)/1024**2} MB\")\n",
    "        print(f\"  Total Memory: {torch.cuda.get_device_properties(i).total_memory/1024**2} MB\")\n",
    "    device = torch.device('cuda:1')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "use_small_dataset = False\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 60000 (total: 60000)\n",
      "Test dataset: 10000 (total: 10000)\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Image size: 784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: figures/images\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset, use_subset=False, subset_fraction=0.1):\n",
    "        \"\"\"\n",
    "        Wrap a dataset with indices\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset: The original dataset\n",
    "        use_subset: Whether to use only a subset of the data\n",
    "        subset_fraction: The fraction of data to use if use_subset is True\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if use_subset:\n",
    "            # Create a subset of indices (10% by default)\n",
    "            total_size = len(dataset)\n",
    "            subset_size = int(total_size * subset_fraction)\n",
    "            \n",
    "            # Create random indices ensuring we get samples from all classes\n",
    "            all_indices = list(range(total_size))\n",
    "            np.random.shuffle(all_indices)\n",
    "            self.indices = all_indices[:subset_size]\n",
    "        else:\n",
    "            # Use all indices\n",
    "            self.indices = list(range(len(dataset)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Map the index to the original dataset index\n",
    "        orig_index = self.indices[index]\n",
    "        data, target = self.dataset[orig_index]\n",
    "        return orig_index, data, target\n",
    "\n",
    "# Create datasets with the option to use only a subset\n",
    "indexed_train_dataset = IndexedDataset(train_dataset, use_subset=use_small_dataset)\n",
    "indexed_test_dataset = IndexedDataset(test_dataset, use_subset=use_small_dataset)\n",
    "\n",
    "# Adjust batch size if using a small dataset\n",
    "batch_size = 128\n",
    "if use_small_dataset:\n",
    "    # Use smaller batch size for small datasets\n",
    "    batch_size = min(batch_size, max(32, len(indexed_train_dataset) // 10))\n",
    "    print(f\"Using small dataset, adjusted batch size: {batch_size}\")\n",
    "\n",
    "train_loader = DataLoader(indexed_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(indexed_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset info\n",
    "print('Train dataset:', len(indexed_train_dataset), \n",
    "      f'({\"10% of \" if use_small_dataset else \"\"}total: {len(train_dataset)})')\n",
    "print('Test dataset:', len(indexed_test_dataset), \n",
    "      f'({\"10% of \" if use_small_dataset else \"\"}total: {len(test_dataset)})')\n",
    "\n",
    "# Print shape of an image\n",
    "print('Image shape:', train_dataset[0][0].shape)\n",
    "# Print total number of pixels in an image\n",
    "print('Image size:', train_dataset[0][0].numel())\n",
    "\n",
    "all_labels = torch.tensor([train_dataset[i][1] for i in indexed_train_dataset.indices], device=device)\n",
    "\n",
    "# Get a batch of images and labels\n",
    "batch_train = next(iter(train_loader))\n",
    "indices_train, images_train, labels_train = batch_train\n",
    "\n",
    "batch_test = next(iter(test_loader))\n",
    "indices_test, images_test, labels_test = batch_test\n",
    "\n",
    "# Plot the images with their labels\n",
    "plot_images(images_train, labels_train, 'Fashion MNIST Train samples', epoch=None, cmap='viridis')\n",
    "plot_images(images_test, labels_test, 'Fashion MNIST Test samples', epoch=None, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder parameters: 247,297 (0.25M)\n",
      "Train representation parameters: 300,000 (0.30M)\n",
      "Test representation parameters: 50,000 (0.05M)\n",
      "Total trainable parameters: 597,297 (0.60M)\n",
      "\n",
      "Parameter distribution:\n",
      "Decoder: 45.2%\n",
      "Train rep: 54.8%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/maps/projects/heads/people/kbh904/ImageDGD/utils/gmm.py:752: UserWarning: EM algorithm did not converge. Try increasing max_iter or lowering tol.\n",
      "  warnings.warn(\"EM algorithm did not converge. Try increasing max_iter or lowering tol.\", UserWarning)\n",
      "/maps/projects/heads/people/kbh904/ImageDGD/utils/gmm.py:689: UserWarning: EM algorithm did not converge. Try increasing max_iter or lowering tol.\n",
      "  warnings.warn(\"EM algorithm did not converge. Try increasing max_iter or lowering tol.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 [TPE: 0:00:13, RT: 0:43:55]; Train Loss: 69.2417 (0.00%), Test Loss: 68.0479 (0.00%), Recon Train: 69.2417 (0.00%), Recon Test: 68.0479 (0.00%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Created directory: figures/latent/pca\n",
      "Unused keyword parameter: max_iter during cuML estimator initialization\n",
      "# of Nearest Neighbors should be at least 3 * perplexity. Your results might be a bit strange...\n",
      "Created directory: figures/latent/tsne\n",
      "build_algo set to brute_force_knn because random_state is given\n",
      "Created directory: figures/latent/umap\n",
      "Epoch 2/200 [TPE: 0:00:12, RT: 0:43:13]; Train Loss: 66.3593 (4.16%), Test Loss: 65.4955 (3.75%), Recon Train: 66.3593 (4.16%), Recon Test: 65.4955 (3.75%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 3/200 [TPE: 0:00:12, RT: 0:42:53]; Train Loss: 60.7030 (8.52%), Test Loss: 60.3213 (7.90%), Recon Train: 60.7030 (8.52%), Recon Test: 60.3213 (7.90%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 4/200 [TPE: 0:00:13, RT: 0:42:38]; Train Loss: 54.5807 (10.09%), Test Loss: 54.1761 (10.19%), Recon Train: 54.5807 (10.09%), Recon Test: 54.1761 (10.19%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 5/200 [TPE: 0:00:12, RT: 0:42:22]; Train Loss: 48.8072 (10.58%), Test Loss: 48.4250 (10.62%), Recon Train: 48.8072 (10.58%), Recon Test: 48.4250 (10.62%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 6/200 [TPE: 0:00:12, RT: 0:42:05]; Train Loss: 43.8954 (10.06%), Test Loss: 43.4012 (10.37%), Recon Train: 43.8954 (10.06%), Recon Test: 43.4012 (10.37%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 7/200 [TPE: 0:00:12, RT: 0:41:40]; Train Loss: 39.8465 (9.22%), Test Loss: 39.2631 (9.53%), Recon Train: 39.8465 (9.22%), Recon Test: 39.2631 (9.53%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 8/200 [TPE: 0:00:12, RT: 0:41:27]; Train Loss: 36.5116 (8.37%), Test Loss: 35.6736 (9.14%), Recon Train: 36.5116 (8.37%), Recon Test: 35.6736 (9.14%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 9/200 [TPE: 0:00:13, RT: 0:41:15]; Train Loss: 33.8251 (7.36%), Test Loss: 33.0512 (7.35%), Recon Train: 33.8251 (7.36%), Recon Test: 33.0512 (7.35%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 10/200 [TPE: 0:00:13, RT: 0:41:04]; Train Loss: 31.7073 (6.26%), Test Loss: 30.6630 (7.23%), Recon Train: 31.7073 (6.26%), Recon Test: 30.6630 (7.23%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 11/200 [TPE: 0:00:13, RT: 0:40:52]; Train Loss: 30.0141 (5.34%), Test Loss: 29.0978 (5.10%), Recon Train: 30.0141 (5.34%), Recon Test: 29.0978 (5.10%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 12/200 [TPE: 0:00:13, RT: 0:40:40]; Train Loss: 28.6435 (4.57%), Test Loss: 27.5896 (5.18%), Recon Train: 28.6435 (4.57%), Recon Test: 27.5896 (5.18%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 13/200 [TPE: 0:00:12, RT: 0:40:26]; Train Loss: 27.5301 (3.89%), Test Loss: 26.5689 (3.70%), Recon Train: 27.5301 (3.89%), Recon Test: 26.5689 (3.70%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 14/200 [TPE: 0:00:12, RT: 0:40:09]; Train Loss: 26.5922 (3.41%), Test Loss: 25.5159 (3.96%), Recon Train: 26.5922 (3.41%), Recon Test: 25.5159 (3.96%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 15/200 [TPE: 0:00:12, RT: 0:39:49]; Train Loss: 25.7810 (3.05%), Test Loss: 24.7035 (3.18%), Recon Train: 25.7810 (3.05%), Recon Test: 24.7035 (3.18%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 16/200 [TPE: 0:00:09, RT: 0:38:58]; Train Loss: 25.0877 (2.69%), Test Loss: 24.0169 (2.78%), Recon Train: 25.0877 (2.69%), Recon Test: 24.0169 (2.78%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 17/200 [TPE: 0:00:09, RT: 0:38:12]; Train Loss: 24.4752 (2.44%), Test Loss: 23.4270 (2.46%), Recon Train: 24.4752 (2.44%), Recon Test: 23.4270 (2.46%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 18/200 [TPE: 0:00:09, RT: 0:37:28]; Train Loss: 23.9190 (2.27%), Test Loss: 22.8646 (2.40%), Recon Train: 23.9190 (2.27%), Recon Test: 22.8646 (2.40%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 19/200 [TPE: 0:00:09, RT: 0:36:48]; Train Loss: 23.4084 (2.13%), Test Loss: 22.5052 (1.57%), Recon Train: 23.4084 (2.13%), Recon Test: 22.5052 (1.57%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 20/200 [TPE: 0:00:12, RT: 0:36:37]; Train Loss: 22.9889 (1.79%), Test Loss: 21.9093 (2.65%), Recon Train: 22.9889 (1.79%), Recon Test: 21.9093 (2.65%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 21/200 [TPE: 0:00:13, RT: 0:36:36]; Train Loss: 22.5247 (2.02%), Test Loss: 21.4046 (2.30%), Recon Train: 22.5247 (2.02%), Recon Test: 21.4046 (2.30%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 22/200 [TPE: 0:00:13, RT: 0:36:36]; Train Loss: 22.1254 (1.77%), Test Loss: 21.0289 (1.75%), Recon Train: 22.1254 (1.77%), Recon Test: 21.0289 (1.75%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 23/200 [TPE: 0:00:13, RT: 0:36:31]; Train Loss: 21.7686 (1.61%), Test Loss: 20.7335 (1.40%), Recon Train: 21.7686 (1.61%), Recon Test: 20.7335 (1.40%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 24/200 [TPE: 0:00:13, RT: 0:36:24]; Train Loss: 21.4113 (1.64%), Test Loss: 20.3200 (1.99%), Recon Train: 21.4113 (1.64%), Recon Test: 20.3200 (1.99%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 25/200 [TPE: 0:00:13, RT: 0:36:19]; Train Loss: 21.0895 (1.50%), Test Loss: 20.1054 (1.06%), Recon Train: 21.0895 (1.50%), Recon Test: 20.1054 (1.06%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 26/200 [TPE: 0:00:13, RT: 0:36:15]; Train Loss: 20.7950 (1.40%), Test Loss: 19.6602 (2.21%), Recon Train: 20.7950 (1.40%), Recon Test: 19.6602 (2.21%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 27/200 [TPE: 0:00:13, RT: 0:36:07]; Train Loss: 20.5377 (1.24%), Test Loss: 19.5888 (0.36%), Recon Train: 20.5377 (1.24%), Recon Test: 19.5888 (0.36%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 28/200 [TPE: 0:00:13, RT: 0:36:02]; Train Loss: 20.2803 (1.25%), Test Loss: 19.1214 (2.39%), Recon Train: 20.2803 (1.25%), Recon Test: 19.1214 (2.39%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 29/200 [TPE: 0:00:13, RT: 0:35:55]; Train Loss: 20.0179 (1.29%), Test Loss: 18.9262 (1.02%), Recon Train: 20.0179 (1.29%), Recon Test: 18.9262 (1.02%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 30/200 [TPE: 0:00:12, RT: 0:35:44]; Train Loss: 19.7771 (1.20%), Test Loss: 18.7480 (0.94%), Recon Train: 19.7771 (1.20%), Recon Test: 18.7480 (0.94%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 31/200 [TPE: 0:00:13, RT: 0:35:38]; Train Loss: 19.6162 (0.81%), Test Loss: 18.4789 (1.44%), Recon Train: 19.6162 (0.81%), Recon Test: 18.4789 (1.44%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 32/200 [TPE: 0:00:13, RT: 0:35:32]; Train Loss: 19.4309 (0.94%), Test Loss: 18.2766 (1.09%), Recon Train: 19.4309 (0.94%), Recon Test: 18.2766 (1.09%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 33/200 [TPE: 0:00:13, RT: 0:35:22]; Train Loss: 19.2104 (1.14%), Test Loss: 18.2278 (0.27%), Recon Train: 19.2104 (1.14%), Recon Test: 18.2278 (0.27%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 34/200 [TPE: 0:00:13, RT: 0:35:13]; Train Loss: 19.0526 (0.82%), Test Loss: 17.9053 (1.77%), Recon Train: 19.0526 (0.82%), Recon Test: 17.9053 (1.77%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 35/200 [TPE: 0:00:13, RT: 0:35:04]; Train Loss: 18.8807 (0.90%), Test Loss: 17.7177 (1.05%), Recon Train: 18.8807 (0.90%), Recon Test: 17.7177 (1.05%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 36/200 [TPE: 0:00:12, RT: 0:34:52]; Train Loss: 18.6986 (0.96%), Test Loss: 17.5002 (1.23%), Recon Train: 18.6986 (0.96%), Recon Test: 17.5002 (1.23%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 37/200 [TPE: 0:00:13, RT: 0:34:40]; Train Loss: 18.5185 (0.96%), Test Loss: 17.3597 (0.80%), Recon Train: 18.5185 (0.96%), Recon Test: 17.3597 (0.80%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 38/200 [TPE: 0:00:13, RT: 0:34:32]; Train Loss: 18.3963 (0.66%), Test Loss: 17.1105 (1.44%), Recon Train: 18.3963 (0.66%), Recon Test: 17.1105 (1.44%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 39/200 [TPE: 0:00:13, RT: 0:34:23]; Train Loss: 18.2193 (0.96%), Test Loss: 17.0378 (0.42%), Recon Train: 18.2193 (0.96%), Recon Test: 17.0378 (0.42%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 40/200 [TPE: 0:00:12, RT: 0:34:10]; Train Loss: 18.0912 (0.70%), Test Loss: 16.8178 (1.29%), Recon Train: 18.0912 (0.70%), Recon Test: 16.8178 (1.29%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 41/200 [TPE: 0:00:13, RT: 0:34:00]; Train Loss: 17.9558 (0.75%), Test Loss: 16.6513 (0.99%), Recon Train: 17.9558 (0.75%), Recon Test: 16.6513 (0.99%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 42/200 [TPE: 0:00:13, RT: 0:33:49]; Train Loss: 17.8067 (0.83%), Test Loss: 16.6667 (-0.09%), Recon Train: 17.8067 (0.83%), Recon Test: 16.6667 (-0.09%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 43/200 [TPE: 0:00:13, RT: 0:33:38]; Train Loss: 17.6697 (0.77%), Test Loss: 16.3809 (1.71%), Recon Train: 17.6697 (0.77%), Recon Test: 16.3809 (1.71%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 44/200 [TPE: 0:00:12, RT: 0:33:25]; Train Loss: 17.5284 (0.80%), Test Loss: 16.3865 (-0.03%), Recon Train: 17.5284 (0.80%), Recon Test: 16.3865 (-0.03%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 45/200 [TPE: 0:00:13, RT: 0:33:15]; Train Loss: 17.4423 (0.49%), Test Loss: 16.1426 (1.49%), Recon Train: 17.4423 (0.49%), Recon Test: 16.1426 (1.49%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 46/200 [TPE: 0:00:13, RT: 0:33:05]; Train Loss: 17.3071 (0.78%), Test Loss: 16.0179 (0.77%), Recon Train: 17.3071 (0.78%), Recon Test: 16.0179 (0.77%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 47/200 [TPE: 0:00:13, RT: 0:32:53]; Train Loss: 17.1895 (0.68%), Test Loss: 15.9890 (0.18%), Recon Train: 17.1895 (0.68%), Recon Test: 15.9890 (0.18%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 48/200 [TPE: 0:00:13, RT: 0:32:43]; Train Loss: 17.0653 (0.72%), Test Loss: 15.8323 (0.98%), Recon Train: 17.0653 (0.72%), Recon Test: 15.8323 (0.98%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 49/200 [TPE: 0:00:13, RT: 0:32:31]; Train Loss: 16.9922 (0.43%), Test Loss: 15.7110 (0.77%), Recon Train: 16.9922 (0.43%), Recon Test: 15.7110 (0.77%), GMM Train: 0.0000 (0.00%), GMM Test: 0.0000 (0.00%)\n",
      "Epoch 50/200 [TPE: 0:00:13, RT: 0:32:21]; Train Loss: 26.6017 (-56.55%), Test Loss: 26.4915 (-68.62%), Recon Train: 16.8762 (0.68%), Recon Test: 15.5599 (0.96%), GMM Train: 9.7255 (0.00%), GMM Test: 10.9316 (0.00%)\n",
      "Created directory: figures/losses\n",
      "Unused keyword parameter: max_iter during cuML estimator initialization\n",
      "# of Nearest Neighbors should be at least 3 * perplexity. Your results might be a bit strange...\n",
      "build_algo set to brute_force_knn because random_state is given\n",
      "Epoch 51/200 [TPE: 0:00:14, RT: 0:32:12]; Train Loss: 11.5562 (56.56%), Test Loss: 10.4401 (60.59%), Recon Train: 16.7649 (0.66%), Recon Test: 15.4559 (0.67%), GMM Train: -5.2087 (153.56%), GMM Test: -5.0157 (145.88%)\n",
      "Epoch 52/200 [TPE: 0:00:13, RT: 0:32:01]; Train Loss: 11.3975 (1.37%), Test Loss: 10.3156 (1.19%), Recon Train: 16.6445 (0.72%), Recon Test: 15.3653 (0.59%), GMM Train: -5.2470 (-0.73%), GMM Test: -5.0497 (-0.68%)\n",
      "Epoch 53/200 [TPE: 0:00:13, RT: 0:31:51]; Train Loss: 11.2776 (1.05%), Test Loss: 10.1409 (1.69%), Recon Train: 16.5768 (0.41%), Recon Test: 15.2383 (0.83%), GMM Train: -5.2992 (-1.00%), GMM Test: -5.0974 (-0.94%)\n",
      "Epoch 54/200 [TPE: 0:00:13, RT: 0:31:40]; Train Loss: 11.1246 (1.36%), Test Loss: 10.0494 (0.90%), Recon Train: 16.4895 (0.53%), Recon Test: 15.2067 (0.21%), GMM Train: -5.3649 (-1.24%), GMM Test: -5.1574 (-1.18%)\n",
      "Epoch 55/200 [TPE: 0:00:13, RT: 0:31:28]; Train Loss: 10.9498 (1.57%), Test Loss: 9.7877 (2.60%), Recon Train: 16.3930 (0.59%), Recon Test: 15.0177 (1.24%), GMM Train: -5.4432 (-1.46%), GMM Test: -5.2300 (-1.41%)\n",
      "Epoch 56/200 [TPE: 0:00:14, RT: 0:31:18]; Train Loss: 10.7691 (1.65%), Test Loss: 9.5913 (2.01%), Recon Train: 16.3030 (0.55%), Recon Test: 14.9047 (0.75%), GMM Train: -5.5339 (-1.67%), GMM Test: -5.3133 (-1.59%)\n",
      "Epoch 57/200 [TPE: 0:00:13, RT: 0:31:07]; Train Loss: 10.5707 (1.84%), Test Loss: 9.4386 (1.59%), Recon Train: 16.2080 (0.58%), Recon Test: 14.8461 (0.39%), GMM Train: -5.6372 (-1.87%), GMM Test: -5.4074 (-1.77%)\n",
      "Epoch 58/200 [TPE: 0:00:13, RT: 0:30:54]; Train Loss: 10.3983 (1.63%), Test Loss: 9.1446 (3.11%), Recon Train: 16.1524 (0.34%), Recon Test: 14.6570 (1.27%), GMM Train: -5.7541 (-2.07%), GMM Test: -5.5124 (-1.94%)\n",
      "Epoch 59/200 [TPE: 0:00:13, RT: 0:30:43]; Train Loss: 10.1785 (2.11%), Test Loss: 9.0658 (0.86%), Recon Train: 16.0632 (0.55%), Recon Test: 14.6959 (-0.27%), GMM Train: -5.8847 (-2.27%), GMM Test: -5.6301 (-2.14%)\n",
      "Epoch 60/200 [TPE: 0:00:14, RT: 0:30:33]; Train Loss: 9.9600 (2.15%), Test Loss: 8.8042 (2.89%), Recon Train: 15.9909 (0.45%), Recon Test: 14.5677 (0.87%), GMM Train: -6.0309 (-2.49%), GMM Test: -5.7635 (-2.37%)\n",
      "Epoch 61/200 [TPE: 0:00:13, RT: 0:30:20]; Train Loss: 9.7478 (2.13%), Test Loss: 8.6553 (1.69%), Recon Train: 15.9352 (0.35%), Recon Test: 14.5605 (0.05%), GMM Train: -6.1875 (-2.60%), GMM Test: -5.9052 (-2.46%)\n",
      "Epoch 62/200 [TPE: 0:00:13, RT: 0:30:09]; Train Loss: 9.5558 (1.97%), Test Loss: 8.4451 (2.43%), Recon Train: 15.9034 (0.20%), Recon Test: 14.4999 (0.42%), GMM Train: -6.3476 (-2.59%), GMM Test: -6.0548 (-2.53%)\n",
      "Epoch 63/200 [TPE: 0:00:13, RT: 0:29:58]; Train Loss: 9.3293 (2.37%), Test Loss: 8.1648 (3.32%), Recon Train: 15.8549 (0.31%), Recon Test: 14.3869 (0.78%), GMM Train: -6.5256 (-2.80%), GMM Test: -6.2221 (-2.76%)\n",
      "Epoch 64/200 [TPE: 0:00:13, RT: 0:29:45]; Train Loss: 9.0993 (2.46%), Test Loss: 7.9795 (2.27%), Recon Train: 15.8301 (0.16%), Recon Test: 14.3959 (-0.06%), GMM Train: -6.7308 (-3.15%), GMM Test: -6.4164 (-3.12%)\n",
      "Epoch 65/200 [TPE: 0:00:13, RT: 0:29:33]; Train Loss: 8.8406 (2.84%), Test Loss: 7.6852 (3.69%), Recon Train: 15.8029 (0.17%), Recon Test: 14.3179 (0.54%), GMM Train: -6.9623 (-3.44%), GMM Test: -6.6327 (-3.37%)\n",
      "Epoch 66/200 [TPE: 0:00:14, RT: 0:29:22]; Train Loss: 8.6253 (2.44%), Test Loss: 7.4961 (2.46%), Recon Train: 15.8039 (-0.01%), Recon Test: 14.3424 (-0.17%), GMM Train: -7.1786 (-3.11%), GMM Test: -6.8464 (-3.22%)\n",
      "Epoch 67/200 [TPE: 0:00:13, RT: 0:29:11]; Train Loss: 8.3846 (2.79%), Test Loss: 7.2349 (3.48%), Recon Train: 15.8018 (0.01%), Recon Test: 14.3193 (0.16%), GMM Train: -7.4172 (-3.32%), GMM Test: -7.0844 (-3.48%)\n",
      "Epoch 68/200 [TPE: 0:00:14, RT: 0:29:00]; Train Loss: 8.1549 (2.74%), Test Loss: 7.0594 (2.43%), Recon Train: 15.8357 (-0.21%), Recon Test: 14.3980 (-0.55%), GMM Train: -7.6808 (-3.55%), GMM Test: -7.3386 (-3.59%)\n",
      "Epoch 69/200 [TPE: 0:00:14, RT: 0:28:48]; Train Loss: 7.8998 (3.13%), Test Loss: 6.7856 (3.88%), Recon Train: 15.8582 (-0.14%), Recon Test: 14.4026 (-0.03%), GMM Train: -7.9584 (-3.61%), GMM Test: -7.6170 (-3.79%)\n",
      "Epoch 70/200 [TPE: 0:00:13, RT: 0:28:36]; Train Loss: 7.6778 (2.81%), Test Loss: 6.5345 (3.70%), Recon Train: 15.9028 (-0.28%), Recon Test: 14.4153 (-0.09%), GMM Train: -8.2250 (-3.35%), GMM Test: -7.8808 (-3.46%)\n",
      "Epoch 71/200 [TPE: 0:00:14, RT: 0:28:25]; Train Loss: 7.4469 (3.01%), Test Loss: 6.3398 (2.98%), Recon Train: 15.9160 (-0.08%), Recon Test: 14.4685 (-0.37%), GMM Train: -8.4691 (-2.97%), GMM Test: -8.1287 (-3.15%)\n",
      "Epoch 72/200 [TPE: 0:00:13, RT: 0:28:13]; Train Loss: 7.2624 (2.48%), Test Loss: 5.9739 (5.77%), Recon Train: 15.9510 (-0.22%), Recon Test: 14.3256 (0.99%), GMM Train: -8.6886 (-2.59%), GMM Test: -8.3517 (-2.74%)\n",
      "Epoch 73/200 [TPE: 0:00:14, RT: 0:28:02]; Train Loss: 7.0391 (3.07%), Test Loss: 5.9203 (0.90%), Recon Train: 15.9733 (-0.14%), Recon Test: 14.5123 (-1.30%), GMM Train: -8.9342 (-2.83%), GMM Test: -8.5921 (-2.88%)\n",
      "Epoch 74/200 [TPE: 0:00:13, RT: 0:27:50]; Train Loss: 6.8267 (3.02%), Test Loss: 5.5122 (6.89%), Recon Train: 16.0154 (-0.26%), Recon Test: 14.3591 (1.06%), GMM Train: -9.1887 (-2.85%), GMM Test: -8.8469 (-2.97%)\n",
      "Epoch 75/200 [TPE: 0:00:14, RT: 0:27:38]; Train Loss: 6.5917 (3.44%), Test Loss: 5.3839 (2.33%), Recon Train: 16.0580 (-0.27%), Recon Test: 14.5054 (-1.02%), GMM Train: -9.4662 (-3.02%), GMM Test: -9.1215 (-3.10%)\n",
      "Epoch 76/200 [TPE: 0:00:14, RT: 0:27:27]; Train Loss: 6.3808 (3.20%), Test Loss: 5.1108 (5.07%), Recon Train: 16.1084 (-0.31%), Recon Test: 14.4805 (0.17%), GMM Train: -9.7277 (-2.76%), GMM Test: -9.3697 (-2.72%)\n",
      "Epoch 77/200 [TPE: 0:00:13, RT: 0:27:14]; Train Loss: 6.1537 (3.56%), Test Loss: 4.8733 (4.65%), Recon Train: 16.1555 (-0.29%), Recon Test: 14.5000 (-0.14%), GMM Train: -10.0018 (-2.82%), GMM Test: -9.6267 (-2.74%)\n",
      "Epoch 78/200 [TPE: 0:00:14, RT: 0:27:02]; Train Loss: 5.9762 (2.88%), Test Loss: 4.7244 (3.06%), Recon Train: 16.2271 (-0.44%), Recon Test: 14.5997 (-0.69%), GMM Train: -10.2509 (-2.49%), GMM Test: -9.8754 (-2.58%)\n",
      "Epoch 79/200 [TPE: 0:00:13, RT: 0:26:50]; Train Loss: 5.7531 (3.73%), Test Loss: 4.4691 (5.40%), Recon Train: 16.2602 (-0.20%), Recon Test: 14.5973 (0.02%), GMM Train: -10.5071 (-2.50%), GMM Test: -10.1283 (-2.56%)\n",
      "Epoch 80/200 [TPE: 0:00:13, RT: 0:26:38]; Train Loss: 5.5242 (3.98%), Test Loss: 4.3097 (3.57%), Recon Train: 16.3196 (-0.37%), Recon Test: 14.7262 (-0.88%), GMM Train: -10.7954 (-2.74%), GMM Test: -10.4165 (-2.85%)\n",
      "Epoch 81/200 [TPE: 0:00:14, RT: 0:26:26]; Train Loss: 5.3168 (3.75%), Test Loss: 4.1858 (2.88%), Recon Train: 16.4123 (-0.57%), Recon Test: 14.9023 (-1.20%), GMM Train: -11.0955 (-2.78%), GMM Test: -10.7166 (-2.88%)\n",
      "Epoch 82/200 [TPE: 0:00:13, RT: 0:26:13]; Train Loss: 5.1361 (3.40%), Test Loss: 3.9265 (6.19%), Recon Train: 16.5277 (-0.70%), Recon Test: 14.9616 (-0.40%), GMM Train: -11.3916 (-2.67%), GMM Test: -11.0351 (-2.97%)\n",
      "Epoch 83/200 [TPE: 0:00:14, RT: 0:26:00]; Train Loss: 4.9487 (3.65%), Test Loss: 4.2995 (-9.50%), Recon Train: 16.6048 (-0.47%), Recon Test: 15.6055 (-4.30%), GMM Train: -11.6560 (-2.32%), GMM Test: -11.3060 (-2.46%)\n",
      "Epoch 84/200 [TPE: 0:00:14, RT: 0:25:48]; Train Loss: 4.7274 (4.47%), Test Loss: 3.5032 (18.52%), Recon Train: 16.6337 (-0.17%), Recon Test: 15.0238 (3.73%), GMM Train: -11.9063 (-2.15%), GMM Test: -11.5206 (-1.90%)\n",
      "Epoch 85/200 [TPE: 0:00:13, RT: 0:25:35]; Train Loss: 4.5583 (3.58%), Test Loss: 3.4097 (2.67%), Recon Train: 16.6871 (-0.32%), Recon Test: 15.1531 (-0.86%), GMM Train: -12.1288 (-1.87%), GMM Test: -11.7434 (-1.93%)\n",
      "Epoch 86/200 [TPE: 0:00:13, RT: 0:25:22]; Train Loss: 4.4339 (2.73%), Test Loss: 3.2593 (4.41%), Recon Train: 16.7470 (-0.36%), Recon Test: 15.1753 (-0.15%), GMM Train: -12.3131 (-1.52%), GMM Test: -11.9160 (-1.47%)\n",
      "Epoch 87/200 [TPE: 0:00:14, RT: 0:25:10]; Train Loss: 4.2622 (3.87%), Test Loss: 3.1037 (4.77%), Recon Train: 16.7433 (0.02%), Recon Test: 15.2501 (-0.49%), GMM Train: -12.4811 (-1.36%), GMM Test: -12.1464 (-1.93%)\n",
      "Epoch 88/200 [TPE: 0:00:13, RT: 0:24:57]; Train Loss: 4.1399 (2.87%), Test Loss: 2.8173 (9.23%), Recon Train: 16.7947 (-0.31%), Recon Test: 15.1407 (0.72%), GMM Train: -12.6548 (-1.39%), GMM Test: -12.3233 (-1.46%)\n",
      "Epoch 89/200 [TPE: 0:00:14, RT: 0:24:45]; Train Loss: 3.9667 (4.18%), Test Loss: 2.8751 (-2.05%), Recon Train: 16.7706 (0.14%), Recon Test: 15.3929 (-1.67%), GMM Train: -12.8039 (-1.18%), GMM Test: -12.5178 (-1.58%)\n",
      "Epoch 90/200 [TPE: 0:00:13, RT: 0:24:32]; Train Loss: 3.8503 (2.93%), Test Loss: 2.5854 (10.08%), Recon Train: 16.8140 (-0.26%), Recon Test: 15.2939 (0.64%), GMM Train: -12.9636 (-1.25%), GMM Test: -12.7085 (-1.52%)\n",
      "Epoch 91/200 [TPE: 0:00:14, RT: 0:24:20]; Train Loss: 3.7308 (3.10%), Test Loss: 2.3601 (8.71%), Recon Train: 16.8424 (-0.17%), Recon Test: 15.2195 (0.49%), GMM Train: -13.1116 (-1.14%), GMM Test: -12.8593 (-1.19%)\n",
      "Epoch 92/200 [TPE: 0:00:14, RT: 0:24:07]; Train Loss: 3.6104 (3.23%), Test Loss: 2.3189 (1.75%), Recon Train: 16.8584 (-0.10%), Recon Test: 15.3064 (-0.57%), GMM Train: -13.2480 (-1.04%), GMM Test: -12.9876 (-1.00%)\n",
      "Epoch 93/200 [TPE: 0:00:13, RT: 0:23:54]; Train Loss: 3.5002 (3.05%), Test Loss: 2.0681 (10.81%), Recon Train: 16.8598 (-0.01%), Recon Test: 15.1835 (0.80%), GMM Train: -13.3596 (-0.84%), GMM Test: -13.1154 (-0.98%)\n",
      "Epoch 94/200 [TPE: 0:00:14, RT: 0:23:42]; Train Loss: 3.3909 (3.12%), Test Loss: 2.1676 (-4.81%), Recon Train: 16.8684 (-0.05%), Recon Test: 15.3874 (-1.34%), GMM Train: -13.4775 (-0.88%), GMM Test: -13.2198 (-0.80%)\n",
      "Epoch 95/200 [TPE: 0:00:13, RT: 0:23:28]; Train Loss: 3.3047 (2.54%), Test Loss: 1.8570 (14.33%), Recon Train: 16.8986 (-0.18%), Recon Test: 15.1882 (1.29%), GMM Train: -13.5938 (-0.86%), GMM Test: -13.3312 (-0.84%)\n",
      "Epoch 96/200 [TPE: 0:00:13, RT: 0:23:15]; Train Loss: 3.2105 (2.85%), Test Loss: 1.8057 (2.76%), Recon Train: 16.9313 (-0.19%), Recon Test: 15.3094 (-0.80%), GMM Train: -13.7208 (-0.93%), GMM Test: -13.5037 (-1.29%)\n",
      "Epoch 97/200 [TPE: 0:00:13, RT: 0:23:02]; Train Loss: 3.1103 (3.12%), Test Loss: 1.7395 (3.67%), Recon Train: 16.9429 (-0.07%), Recon Test: 15.3113 (-0.01%), GMM Train: -13.8327 (-0.82%), GMM Test: -13.5718 (-0.50%)\n",
      "Epoch 98/200 [TPE: 0:00:13, RT: 0:22:49]; Train Loss: 3.0080 (3.29%), Test Loss: 1.6031 (7.84%), Recon Train: 16.9428 (0.00%), Recon Test: 15.3605 (-0.32%), GMM Train: -13.9349 (-0.74%), GMM Test: -13.7575 (-1.37%)\n",
      "Epoch 99/200 [TPE: 0:00:13, RT: 0:22:36]; Train Loss: 2.9641 (1.46%), Test Loss: 1.4345 (10.52%), Recon Train: 16.9870 (-0.26%), Recon Test: 15.2503 (0.72%), GMM Train: -14.0229 (-0.63%), GMM Test: -13.8159 (-0.42%)\n",
      "Epoch 100/200 [TPE: 0:00:14, RT: 0:22:23]; Train Loss: 3.9875 (-34.53%), Test Loss: 2.4809 (-72.95%), Recon Train: 16.9783 (0.05%), Recon Test: 15.3543 (-0.68%), GMM Train: -12.9908 (7.36%), GMM Test: -12.8735 (6.82%)\n",
      "Created directory: figures/gmm\n",
      "Unused keyword parameter: max_iter during cuML estimator initialization\n",
      "# of Nearest Neighbors should be at least 3 * perplexity. Your results might be a bit strange...\n",
      "build_algo set to brute_force_knn because random_state is given\n",
      "Epoch 101/200 [TPE: 0:00:14, RT: 0:22:10]; Train Loss: 3.8807 (2.68%), Test Loss: 2.4254 (2.23%), Recon Train: 16.9661 (0.07%), Recon Test: 15.3868 (-0.21%), GMM Train: -13.0854 (-0.73%), GMM Test: -12.9614 (-0.68%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 396\u001b[39m\n\u001b[32m    392\u001b[39m optimizers = [decoder_optimizer, trainrep_optimizer, testrep_optimizer]\n\u001b[32m    394\u001b[39m visualizer = LatentSpaceVisualizer()\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m trained_model, trained_rep, trained_test_rep = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_rep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfirst_epoch_gmm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_epoch_gmm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefit_gmm_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefit_gmm_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_gmm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlambda_gmm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplot_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot_interval\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(train_loader, test_loader, decoder_model, optimizers, test_rep, n_epochs, first_epoch_gmm, refit_gmm_interval, lambda_gmm, metrics_list, device, plot_interval)\u001b[39m\n\u001b[32m     71\u001b[39m model.train()\n\u001b[32m     72\u001b[39m rep_optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rapids/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rapids/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rapids/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mIndexedDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Map the index to the original dataset index\u001b[39;00m\n\u001b[32m     39\u001b[39m     orig_index = \u001b[38;5;28mself\u001b[39m.indices[index]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     data, target = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43morig_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m orig_index, data, target\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rapids/lib/python3.12/site-packages/torchvision/datasets/mnist.py:139\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) -> Tuple[Any, Any]:\n\u001b[32m    132\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m        index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     img, target = \u001b[38;5;28mself\u001b[39m.data[index], \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[32m    143\u001b[39m     img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_model(train_loader, test_loader, decoder_model, optimizers, \n",
    "                          test_rep, n_epochs, first_epoch_gmm=1, \n",
    "                          refit_gmm_interval=None, lambda_gmm=1.0, \n",
    "                          metrics_list=None, device='cuda', plot_interval=10):\n",
    "    \"\"\"\n",
    "    Training loop for the decoder-GMM model with separate train and test representation layers.\n",
    "    \"\"\"\n",
    "    model_optimizer, rep_optimizer, testrep_optimizer = optimizers\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    gmm_train_losses = []\n",
    "    gmm_test_losses = []\n",
    "    recon_train_losses = []\n",
    "    recon_test_losses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_times = []  # Track epoch times for better RT calculation\n",
    "    \n",
    "    # Get references from the decoder model\n",
    "    model = decoder_model.decoder\n",
    "    rep = decoder_model.rep_layer\n",
    "    gmm = decoder_model.gmm\n",
    "    \n",
    "    def calc_improvement(loss_list):\n",
    "        # Returns the percentage improvement compared to the previous epoch\n",
    "        if len(loss_list) < 2:\n",
    "            return 0.0\n",
    "        previous = loss_list[-2]\n",
    "        current = loss_list[-1]\n",
    "        return ((previous - current) / previous) * 100 if previous != 0 else 0.0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train_losses.append(0)\n",
    "        test_losses.append(0)\n",
    "        gmm_train_losses.append(0)\n",
    "        gmm_test_losses.append(0)\n",
    "        recon_train_losses.append(0)\n",
    "        recon_test_losses.append(0)\n",
    "\n",
    "        # Initialize or refit GMM\n",
    "        if epoch == 1 and not (first_epoch_gmm == 1):\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm.fit(representations, max_iter=1)\n",
    "        elif (epoch == first_epoch_gmm) or (refit_gmm_interval and epoch % refit_gmm_interval == 0):\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm = GaussianMixture(\n",
    "                    n_features=rep.z.shape[1], \n",
    "                    n_components=n_components, \n",
    "                    covariance_type=covariance_type, \n",
    "                    init_params=init_params,\n",
    "                    device=device, \n",
    "                    random_state=RANDOM_STATE, \n",
    "                    verbose=verbose, \n",
    "                    max_iter=max_iter,\n",
    "                    tol=tol,\n",
    "                    n_init=n_init,\n",
    "                    warm_start=True\n",
    "                )\n",
    "                gmm.fit(representations, max_iter=1000)\n",
    "        elif epoch > first_epoch_gmm:\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm.fit(representations, max_iter=100, warm_start=True)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        rep_optimizer.zero_grad()\n",
    "        \n",
    "        for i, (index, x, labels_batch) in enumerate(train_loader):\n",
    "            model_optimizer.zero_grad()\n",
    "            \n",
    "            x, index = x.to(device), index.to(device)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                # Using GMM\n",
    "                z = rep(index)\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss + gmm_error\n",
    "            else:\n",
    "                # Without GMM\n",
    "                z = rep(index)\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                loss = recon_loss\n",
    "                gmm_error = torch.tensor(0.0).to(device)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            train_losses[-1] += loss.item()\n",
    "            recon_train_losses[-1] += recon_loss.item()\n",
    "            if epoch >= first_epoch_gmm:\n",
    "                gmm_train_losses[-1] += gmm_error.item()\n",
    "        \n",
    "        rep_optimizer.step()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Testing loop\n",
    "        model.eval()\n",
    "        testrep_optimizer.zero_grad()  # Zero out test rep gradients\n",
    "        \n",
    "        for i, (index, x, _) in enumerate(test_loader):\n",
    "            x, index = x.to(device), index.to(device)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                # Using GMM\n",
    "                z = test_rep(index)  # Use test_rep\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss + gmm_error\n",
    "                \n",
    "                # Backpropagate for test_rep\n",
    "                loss.backward()\n",
    "            else:\n",
    "                # Without GMM\n",
    "                z = test_rep(index)  # Use test_rep\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                loss = recon_loss\n",
    "                gmm_error = torch.tensor(0.0).to(device)\n",
    "                \n",
    "                # Backpropagate for test_rep\n",
    "                loss.backward()\n",
    "            \n",
    "            # Track losses\n",
    "            test_losses[-1] += loss.item()\n",
    "            recon_test_losses[-1] += recon_loss.item()\n",
    "            if epoch >= first_epoch_gmm:\n",
    "                gmm_test_losses[-1] += gmm_error.item()\n",
    "        \n",
    "        testrep_optimizer.step()  # Update test_rep parameters\n",
    "        \n",
    "        # Normalize losses by dataset size\n",
    "        train_losses[-1] /= len(train_loader.dataset)\n",
    "        test_losses[-1] /= len(test_loader.dataset)\n",
    "        recon_train_losses[-1] /= len(train_loader.dataset)\n",
    "        recon_test_losses[-1] /= len(test_loader.dataset)\n",
    "        gmm_train_losses[-1] /= len(train_loader.dataset)\n",
    "        gmm_test_losses[-1] /= len(test_loader.dataset)\n",
    "        \n",
    "        # Calculate timing information\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_duration)\n",
    "        \n",
    "        # Calculate estimated time remaining\n",
    "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_epochs = n_epochs - epoch\n",
    "        estimated_time_remaining = remaining_epochs * avg_epoch_time\n",
    "        \n",
    "        # Format the time for display\n",
    "        epoch_time_str = str(timedelta(seconds=int(epoch_duration)))\n",
    "        remaining_time_str = str(timedelta(seconds=int(estimated_time_remaining)))\n",
    "        \n",
    "        # Print epoch statistics with timing information\n",
    "        print(f\"Epoch {epoch}/{n_epochs} \"\n",
    "              f\"[TPE: {epoch_time_str}, RT: {remaining_time_str}]; \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f} ({calc_improvement(train_losses):.2f}%), \"\n",
    "              f\"Test Loss: {test_losses[-1]:.4f} ({calc_improvement(test_losses):.2f}%), \"\n",
    "              f\"Recon Train: {recon_train_losses[-1]:.4f} ({calc_improvement(recon_train_losses):.2f}%), \"\n",
    "              f\"Recon Test: {recon_test_losses[-1]:.4f} ({calc_improvement(recon_test_losses):.2f}%), \"\n",
    "              f\"GMM Train: {gmm_train_losses[-1]:.4f} ({calc_improvement(gmm_train_losses):.2f}%), \"\n",
    "              f\"GMM Test: {gmm_test_losses[-1]:.4f} ({calc_improvement(gmm_test_losses):.2f}%)\")\n",
    "        \n",
    "        # Plot losses at regular intervals\n",
    "        if epoch % plot_interval == 0 or epoch == n_epochs:\n",
    "            plot_training_losses(\n",
    "                train_losses, test_losses,\n",
    "                recon_train_losses, recon_test_losses,\n",
    "                gmm_train_losses, gmm_test_losses,\n",
    "                title=f\"Training Losses at Epoch {epoch}\"\n",
    "            )\n",
    "\n",
    "        if epoch % plot_interval == 0 or epoch == n_epochs:\n",
    "            with torch.no_grad():\n",
    "                # Plot reconstructed training images\n",
    "                z_train = rep(indices_train.to(device))\n",
    "                reconstructions_train = model(z_train)\n",
    "                reconstructions_train = reconstructions_train.cpu()\n",
    "                plot_images(reconstructions_train, labels_train.cpu(), \"Reconstructed Train Images\", epoch=epoch)\n",
    "                \n",
    "                # Plot reconstructed test images\n",
    "                z_test = test_rep(indices_test.to(device))\n",
    "                reconstructions_test = model(z_test)\n",
    "                reconstructions_test = reconstructions_test.cpu()\n",
    "                plot_images(reconstructions_test, labels_test.cpu(), \"Reconstructed Test Images\", epoch=epoch)\n",
    "\n",
    "                # Plot GMM component reconstructions (only after GMM is fitted)\n",
    "                if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                    plot_gmm_images(\n",
    "                        model, gmm, \"GMM Component Means (by weight)\",\n",
    "                        epoch=epoch, top_n=n_components, device=device\n",
    "                    )\n",
    "                    plot_gmm_samples(\n",
    "                        model, gmm, \"Generated Images from GMM Samples\",\n",
    "                        n_samples=n_components, epoch=epoch, device=device\n",
    "                    )\n",
    "\n",
    "        # Plot latent space visualizations\n",
    "        if gmm is not None:\n",
    "            if epoch % plot_interval == 0 or epoch == n_epochs or epoch == 1 or epoch == first_epoch_gmm:\n",
    "                with torch.no_grad():\n",
    "                    z_train = rep.z.detach()\n",
    "                    z_test = test_rep.z.detach()\n",
    "                    labels_train = torch.tensor([label for _, _, label in indexed_train_dataset])\n",
    "                    labels_test = torch.tensor([label for _, _, label in indexed_test_dataset])\n",
    "\n",
    "                    label_names = [\n",
    "                        \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                        \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "                    ]\n",
    "\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='pca', \n",
    "                                    title=\"Latent Space - PCA\",\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "                    # visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                    #                 method='kpca', \n",
    "                    #                 kernel='rbf',\n",
    "                    #                 title=\"Latent Space - Kernel PCA\",\n",
    "                    #                 label_names=label_names,\n",
    "                    #                 epoch=epoch)\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='tsne', \n",
    "                                    perplexity=30,\n",
    "                                    n_iter=1000,\n",
    "                                    title=\"Latent Space - t-SNE\",\n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='umap',\n",
    "                                    n_neighbors=20,\n",
    "                                    n_components=2,\n",
    "                                    min_dist=0.01,\n",
    "                                    title=\"Latent Space - UMAP\",\n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "        \n",
    "    return decoder_model, rep, test_rep\n",
    "\n",
    "# Initialize components\n",
    "nsample_train = len(indexed_train_dataset)\n",
    "nsample_test = len(indexed_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GMM Parameters\n",
    "n_features = 5\n",
    "n_components = 20\n",
    "covariance_type = 'diag'\n",
    "init_params = 'kmeans'\n",
    "max_iter = 1000\n",
    "tol = 1e-4\n",
    "warm_start = True\n",
    "verbose = False\n",
    "verbose_interval = 10\n",
    "n_init = 1\n",
    "\n",
    "\n",
    "\n",
    "# Representation Layer Parameters\n",
    "dist = 'uniform_ball'\n",
    "dist_options_train = {\n",
    "    \"n_samples\": nsample_train,\n",
    "    \"dim\": n_features,\n",
    "    \"radius\": 0.1,\n",
    "}\n",
    "dist_options_test = {\n",
    "    \"n_samples\": nsample_test,\n",
    "    \"dim\": n_features,\n",
    "    \"radius\": 0.1,\n",
    "}\n",
    "\n",
    "# Decoder Parameters\n",
    "decoder_hidden_dims = [128, 64, 32]\n",
    "decoder_output_channels = 1\n",
    "decoder_output_size = (28, 28)\n",
    "decoder_activation = 'leaky_relu'\n",
    "decoder_final_activation = 'sigmoid'\n",
    "decoder_use_batch_norm = True\n",
    "decoder_dropout_rate = 0.1\n",
    "decoder_init_size = (7, 7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 200\n",
    "first_epoch_gmm = 50\n",
    "refit_gmm_interval = 100\n",
    "lambda_gmm = 1.0\n",
    "\n",
    "# Representation Layer Optimizer\n",
    "rep_lr = 0.01\n",
    "\n",
    "# Decoder Optimizer\n",
    "decoder_lr = 0.001\n",
    "\n",
    "\n",
    "# Plotting Parameters\n",
    "plot_interval = 50\n",
    "\n",
    "# Create the representation layer\n",
    "rep = RepresentationLayer(dist=dist, dist_options=dist_options_train, device=device)\n",
    "test_rep = RepresentationLayer(dist=dist, dist_options=dist_options_test, device=device)\n",
    "\n",
    "\n",
    "# Create GMM\n",
    "gmm = GaussianMixture(\n",
    "    n_features=n_features, \n",
    "    n_components=n_components, \n",
    "    covariance_type=covariance_type, \n",
    "    init_params=init_params,\n",
    "    device=device, \n",
    "    random_state=RANDOM_STATE, \n",
    "    verbose=False, \n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    n_init=n_init,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "# Create the decoder\n",
    "decoder = ConvDecoder(\n",
    "    latent_dim=n_features,\n",
    "    hidden_dims=decoder_hidden_dims,\n",
    "    output_channels=decoder_output_channels,\n",
    "    output_size=decoder_output_size,\n",
    "    use_batch_norm=decoder_use_batch_norm,\n",
    "    activation=decoder_activation,\n",
    "    final_activation=decoder_final_activation,\n",
    "    dropout_rate=decoder_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# Create the full model\n",
    "decoder_model = DGD(decoder, rep, gmm)\n",
    "\n",
    "\n",
    "# Count and print parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print parameter counts\n",
    "decoder_params = count_parameters(decoder)\n",
    "rep_params = count_parameters(rep)\n",
    "test_rep_params = count_parameters(test_rep)\n",
    "\n",
    "print(f\"Decoder parameters: {decoder_params:,} ({decoder_params / 1e6:.2f}M)\")\n",
    "print(f\"Train representation parameters: {rep_params:,} ({rep_params / 1e6:.2f}M)\")\n",
    "print(f\"Test representation parameters: {test_rep_params:,} ({test_rep_params / 1e6:.2f}M)\")\n",
    "print(f\"Total trainable parameters: {decoder_params + rep_params + test_rep_params:,} ({(decoder_params + rep_params + test_rep_params) / 1e6:.2f}M)\")\n",
    "\n",
    "# Analyze parameter distribution\n",
    "print(\"\\nParameter distribution:\")\n",
    "print(f\"Decoder: {decoder_params / (decoder_params + rep_params) * 100:.1f}%\")\n",
    "print(f\"Train rep: {rep_params / (decoder_params + rep_params) * 100:.1f}%\\n\")\n",
    "\n",
    "\n",
    "# Initialize optimizers\n",
    "decoder_optimizer = torch.optim.AdamW(\n",
    "    decoder.parameters(), \n",
    "    lr=decoder_lr, \n",
    ")\n",
    "trainrep_optimizer = torch.optim.AdamW(\n",
    "    rep.parameters(),\n",
    "    lr=rep_lr,\n",
    ")\n",
    "testrep_optimizer = torch.optim.AdamW(\n",
    "    test_rep.parameters(),\n",
    "    lr=rep_lr,\n",
    ")\n",
    "\n",
    "optimizers = [decoder_optimizer, trainrep_optimizer, testrep_optimizer]\n",
    "\n",
    "visualizer = LatentSpaceVisualizer()\n",
    "\n",
    "trained_model, trained_rep, trained_test_rep = train_model(\n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    decoder_model, \n",
    "    optimizers, \n",
    "    test_rep=test_rep, \n",
    "    n_epochs=epochs, \n",
    "    first_epoch_gmm=first_epoch_gmm,\n",
    "    refit_gmm_interval=refit_gmm_interval,\n",
    "    lambda_gmm=lambda_gmm,\n",
    "    device=device,\n",
    "    plot_interval=plot_interval\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
