{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuML: Installed accelerator for sklearn.\n",
      "cuML: Installed accelerator for umap.\n",
      "cuML: Successfully initialized accelerator.\n",
      "PyTorch CUDA version: 12.6\n",
      "PyTorch version: 2.7.0+cu126\n",
      "Number of CUDA devices: 1\n",
      "Device 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "  Memory Allocated: 0.0 MB\n",
      "  Memory Reserved: 0.0 MB\n",
      "  Total Memory: 7805.5625 MB\n",
      "Using device: cuda:0\n",
      "PyTorch CUDA version: 12.6\n",
      "PyTorch version: 2.7.0+cu126\n",
      "Number of CUDA devices: 1\n",
      "Device 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "  Memory Allocated: 0.0 MB\n",
      "  Memory Reserved: 0.0 MB\n",
      "  Total Memory: 7805.5625 MB\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asp/miniforge3/envs/rapids-25.04/lib/python3.12/site-packages/torch/cuda/__init__.py:789: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import umap\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from datetime import timedelta\n",
    "#import torch_optimizer as optim\n",
    "\n",
    "try:\n",
    "    from cuml.accel import install\n",
    "    install()\n",
    "except ImportError:\n",
    "    print(\"Cuml not installed, using CPU for umap and sklearn\")\n",
    "\n",
    "from src import RepresentationLayer, DGD, ConvDecoder, LatentSpaceVisualizer, GaussianMixture, plot_training_losses, plot_images, plot_gmm_images, plot_gmm_samples\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i)/1024**2} MB\")\n",
    "        print(f\"  Memory Reserved: {torch.cuda.memory_reserved(i)/1024**2} MB\")\n",
    "        print(f\"  Total Memory: {torch.cuda.get_device_properties(i).total_memory/1024**2} MB\")\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "use_small_dataset = False\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 60000 (total: 60000)\n",
      "Test dataset: 10000 (total: 10000)\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Image size: 784\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset, use_subset=False, subset_fraction=0.1):\n",
    "        \"\"\"\n",
    "        Wrap a dataset with indices\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset: The original dataset\n",
    "        use_subset: Whether to use only a subset of the data\n",
    "        subset_fraction: The fraction of data to use if use_subset is True\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if use_subset:\n",
    "            # Create a subset of indices (10% by default)\n",
    "            total_size = len(dataset)\n",
    "            subset_size = int(total_size * subset_fraction)\n",
    "            \n",
    "            # Create random indices ensuring we get samples from all classes\n",
    "            all_indices = list(range(total_size))\n",
    "            np.random.shuffle(all_indices)\n",
    "            self.indices = all_indices[:subset_size]\n",
    "        else:\n",
    "            # Use all indices\n",
    "            self.indices = list(range(len(dataset)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Map the index to the original dataset index\n",
    "        orig_index = self.indices[index]\n",
    "        data, target = self.dataset[orig_index]\n",
    "        return orig_index, data, target\n",
    "\n",
    "# Create datasets with the option to use only a subset\n",
    "indexed_train_dataset = IndexedDataset(train_dataset, use_subset=use_small_dataset)\n",
    "indexed_test_dataset = IndexedDataset(test_dataset, use_subset=use_small_dataset)\n",
    "\n",
    "# Adjust batch size if using a small dataset\n",
    "batch_size = 128\n",
    "if use_small_dataset:\n",
    "    # Use smaller batch size for small datasets\n",
    "    batch_size = min(batch_size, max(32, len(indexed_train_dataset) // 10))\n",
    "    print(f\"Using small dataset, adjusted batch size: {batch_size}\")\n",
    "\n",
    "train_loader = DataLoader(indexed_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(indexed_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset info\n",
    "print('Train dataset:', len(indexed_train_dataset), \n",
    "      f'({\"10% of \" if use_small_dataset else \"\"}total: {len(train_dataset)})')\n",
    "print('Test dataset:', len(indexed_test_dataset), \n",
    "      f'({\"10% of \" if use_small_dataset else \"\"}total: {len(test_dataset)})')\n",
    "\n",
    "# Print shape of an image\n",
    "print('Image shape:', train_dataset[0][0].shape)\n",
    "# Print total number of pixels in an image\n",
    "print('Image size:', train_dataset[0][0].numel())\n",
    "\n",
    "all_labels = torch.tensor([train_dataset[i][1] for i in indexed_train_dataset.indices], device=device)\n",
    "\n",
    "# Get a batch of images and labels\n",
    "batch_train = next(iter(train_loader))\n",
    "indices_train, images_train, labels_train = batch_train\n",
    "\n",
    "batch_test = next(iter(test_loader))\n",
    "indices_test, images_test, labels_test = batch_test\n",
    "\n",
    "# Plot the images with their labels\n",
    "plot_images(images_train, labels_train, 'Fashion MNIST Train samples', epoch=None, cmap='viridis')\n",
    "plot_images(images_test, labels_test, 'Fashion MNIST Test samples', epoch=None, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder parameters: 247,297 (0.25M)\n",
      "Train representation parameters: 300,000 (0.30M)\n",
      "Test representation parameters: 50,000 (0.05M)\n",
      "Total trainable parameters: 597,297 (0.60M)\n",
      "\n",
      "Parameter distribution:\n",
      "Decoder: 45.2%\n",
      "Train rep: 54.8%\n",
      "\n",
      "=== Training Configuration ===\n",
      "Epochs: 200\n",
      "Device: cuda:0\n",
      "First GMM epoch: 50\n",
      "Plot interval: 50\n",
      "Using subset: False\n",
      "Verbose mode: True\n",
      "==============================\n",
      "Initializing GMM at epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asp/Downloads/HeaDS/ImageDGD/src/models/gmm.py:752: UserWarning: EM algorithm did not converge. Try increasing max_iter or lowering tol.\n",
      "  warnings.warn(\"EM algorithm did not converge. Try increasing max_iter or lowering tol.\", UserWarning)\n",
      "/home/asp/Downloads/HeaDS/ImageDGD/src/models/gmm.py:689: UserWarning: EM algorithm did not converge. Try increasing max_iter or lowering tol.\n",
      "  warnings.warn(\"EM algorithm did not converge. Try increasing max_iter or lowering tol.\", UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 450\u001b[39m\n\u001b[32m    446\u001b[39m optimizers = [decoder_optimizer, trainrep_optimizer, testrep_optimizer]\n\u001b[32m    448\u001b[39m visualizer = LatentSpaceVisualizer()\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m trained_model, trained_rep, trained_test_rep = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_rep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfirst_epoch_gmm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_epoch_gmm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefit_gmm_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefit_gmm_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_gmm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlambda_gmm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplot_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot_interval\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(train_loader, test_loader, decoder_model, optimizers, test_rep, n_epochs, first_epoch_gmm, refit_gmm_interval, lambda_gmm, metrics_list, device, plot_interval, use_subset, verbose)\u001b[39m\n\u001b[32m    119\u001b[39m model_optimizer.step()\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Track losses\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m train_losses[-\u001b[32m1\u001b[39m] += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m recon_train_losses[-\u001b[32m1\u001b[39m] += recon_loss.item()\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch >= first_epoch_gmm:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_model(train_loader, test_loader, decoder_model, optimizers, \n",
    "                          test_rep, n_epochs, first_epoch_gmm=1, \n",
    "                          refit_gmm_interval=None, lambda_gmm=1.0, \n",
    "                          metrics_list=None, device='cuda', plot_interval=10,\n",
    "                          use_subset=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Training loop for the decoder-GMM model with separate train and test representation layers.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    use_subset: If True, only shows that subset mode is enabled (data subset is handled by IndexedDataset)\n",
    "    verbose: If True, shows detailed training output with all loss components and timing\n",
    "    \"\"\"\n",
    "    model_optimizer, rep_optimizer, testrep_optimizer = optimizers\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    gmm_train_losses = []\n",
    "    gmm_test_losses = []\n",
    "    recon_train_losses = []\n",
    "    recon_test_losses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_times = []  # Track epoch times for better RT calculation\n",
    "    \n",
    "    # Get references from the decoder model\n",
    "    model = decoder_model.decoder\n",
    "    rep = decoder_model.rep_layer\n",
    "    gmm = decoder_model.gmm\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"=== Training Configuration ===\")\n",
    "        print(f\"Epochs: {n_epochs}\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"First GMM epoch: {first_epoch_gmm}\")\n",
    "        print(f\"Plot interval: {plot_interval}\")\n",
    "        print(f\"Using subset: {use_subset}\")\n",
    "        print(f\"Verbose mode: {verbose}\")\n",
    "        print(\"=\" * 30)\n",
    "    \n",
    "    def calc_improvement(loss_list):\n",
    "        # Returns the percentage improvement compared to the previous epoch\n",
    "        if len(loss_list) < 2:\n",
    "            return 0.0\n",
    "        previous = loss_list[-2]\n",
    "        current = loss_list[-1]\n",
    "        return ((previous - current) / previous) * 100 if previous != 0 else 0.0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train_losses.append(0)\n",
    "        test_losses.append(0)\n",
    "        gmm_train_losses.append(0)\n",
    "        gmm_test_losses.append(0)\n",
    "        recon_train_losses.append(0)\n",
    "        recon_test_losses.append(0)\n",
    "\n",
    "        # Initialize or refit GMM\n",
    "        if epoch == 1 and not (first_epoch_gmm == 1):\n",
    "            if verbose:\n",
    "                print(f\"Initializing GMM at epoch {epoch}...\")\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm.fit(representations, max_iter=1)\n",
    "        elif (epoch == first_epoch_gmm) or (refit_gmm_interval and epoch % refit_gmm_interval == 0):\n",
    "            if verbose:\n",
    "                print(f\"Fitting GMM at epoch {epoch}...\")\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm = GaussianMixture(\n",
    "                    n_features=rep.z.shape[1], \n",
    "                    n_components=n_components, \n",
    "                    covariance_type=covariance_type, \n",
    "                    init_params=init_params,\n",
    "                    device=device, \n",
    "                    random_state=RANDOM_STATE, \n",
    "                    verbose=verbose and (epoch == first_epoch_gmm), \n",
    "                    max_iter=max_iter,\n",
    "                    tol=tol,\n",
    "                    n_init=n_init,\n",
    "                    warm_start=True\n",
    "                )\n",
    "                gmm.fit(representations, max_iter=1000)\n",
    "        elif epoch > first_epoch_gmm:\n",
    "            if verbose and refit_gmm_interval and epoch % refit_gmm_interval == 0:\n",
    "                print(f\"Updating GMM at epoch {epoch}...\")\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach()\n",
    "                gmm.fit(representations, max_iter=100, warm_start=True)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        rep_optimizer.zero_grad()\n",
    "        \n",
    "        for i, (index, x, labels_batch) in enumerate(train_loader):\n",
    "            model_optimizer.zero_grad()\n",
    "            \n",
    "            x, index = x.to(device), index.to(device)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                # Using GMM\n",
    "                z = rep(index)\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss + gmm_error\n",
    "            else:\n",
    "                # Without GMM\n",
    "                z = rep(index)\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                loss = recon_loss\n",
    "                gmm_error = torch.tensor(0.0).to(device)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            train_losses[-1] += loss.item()\n",
    "            recon_train_losses[-1] += recon_loss.item()\n",
    "            if epoch >= first_epoch_gmm:\n",
    "                gmm_train_losses[-1] += gmm_error.item()\n",
    "        \n",
    "        rep_optimizer.step()\n",
    "\n",
    "        # Testing loop\n",
    "        model.eval()\n",
    "        testrep_optimizer.zero_grad()  # Zero out test rep gradients\n",
    "        \n",
    "        for i, (index, x, _) in enumerate(test_loader):\n",
    "            x, index = x.to(device), index.to(device)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                # Using GMM\n",
    "                z = test_rep(index)  # Use test_rep\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss + gmm_error\n",
    "                \n",
    "                # Backpropagate for test_rep\n",
    "                loss.backward()\n",
    "            else:\n",
    "                # Without GMM\n",
    "                z = test_rep(index)  # Use test_rep\n",
    "                y = model(z)\n",
    "                recon_loss = F.mse_loss(y, x, reduction='sum')\n",
    "                loss = recon_loss\n",
    "                gmm_error = torch.tensor(0.0).to(device)\n",
    "                \n",
    "                # Backpropagate for test_rep\n",
    "                loss.backward()\n",
    "            \n",
    "            # Track losses\n",
    "            test_losses[-1] += loss.item()\n",
    "            recon_test_losses[-1] += recon_loss.item()\n",
    "            if epoch >= first_epoch_gmm:\n",
    "                gmm_test_losses[-1] += gmm_error.item()\n",
    "        \n",
    "        testrep_optimizer.step()  # Update test_rep parameters\n",
    "        \n",
    "        # Normalize losses by dataset size\n",
    "        train_losses[-1] /= len(train_loader.dataset)\n",
    "        test_losses[-1] /= len(test_loader.dataset)\n",
    "        recon_train_losses[-1] /= len(train_loader.dataset)\n",
    "        recon_test_losses[-1] /= len(test_loader.dataset)\n",
    "        gmm_train_losses[-1] /= len(train_loader.dataset)\n",
    "        gmm_test_losses[-1] /= len(test_loader.dataset)\n",
    "        \n",
    "        # Calculate timing information\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_duration)\n",
    "        \n",
    "        # Calculate estimated time remaining\n",
    "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_epochs = n_epochs - epoch\n",
    "        estimated_time_remaining = remaining_epochs * avg_epoch_time\n",
    "        \n",
    "        # Format the time for display\n",
    "        epoch_time_str = str(timedelta(seconds=int(epoch_duration)))\n",
    "        remaining_time_str = str(timedelta(seconds=int(estimated_time_remaining)))\n",
    "        \n",
    "        # Print epoch statistics with configurable verbosity\n",
    "        if verbose:\n",
    "            # Detailed output with all components\n",
    "            print(f\"Epoch {epoch}/{n_epochs} \"\n",
    "                  f\"[TPE: {epoch_time_str}, RT: {remaining_time_str}]; \"\n",
    "                  f\"Train Loss: {train_losses[-1]:.4f} ({calc_improvement(train_losses):.2f}%), \"\n",
    "                  f\"Test Loss: {test_losses[-1]:.4f} ({calc_improvement(test_losses):.2f}%), \"\n",
    "                  f\"Recon Train: {recon_train_losses[-1]:.4f} ({calc_improvement(recon_train_losses):.2f}%), \"\n",
    "                  f\"Recon Test: {recon_test_losses[-1]:.4f} ({calc_improvement(recon_test_losses):.2f}%), \"\n",
    "                  f\"GMM Train: {gmm_train_losses[-1]:.4f} ({calc_improvement(gmm_train_losses):.2f}%), \"\n",
    "                  f\"GMM Test: {gmm_test_losses[-1]:.4f} ({calc_improvement(gmm_test_losses):.2f}%)\")\n",
    "        else:\n",
    "            # Compact output\n",
    "            if epoch % 10 == 0 or epoch == n_epochs:\n",
    "                print(f\"Epoch {epoch}/{n_epochs}: \"\n",
    "                      f\"Train={train_losses[-1]:.4f}, Test={test_losses[-1]:.4f}, \"\n",
    "                      f\"Time={epoch_time_str}\")\n",
    "        \n",
    "        # Plot losses at regular intervals (only if verbose or at key epochs)\n",
    "        if (verbose and (epoch % plot_interval == 0 or epoch == n_epochs)) or \\\n",
    "           (not verbose and epoch == n_epochs):\n",
    "            plot_training_losses(\n",
    "                train_losses, test_losses,\n",
    "                recon_train_losses, recon_test_losses,\n",
    "                gmm_train_losses, gmm_test_losses,\n",
    "                title=f\"Training Losses at Epoch {epoch}\"\n",
    "            )\n",
    "\n",
    "        # Plot images and visualizations (reduced frequency for non-verbose)\n",
    "        plot_this_epoch = (verbose and (epoch % plot_interval == 0 or epoch == n_epochs)) or \\\n",
    "                         (not verbose and (epoch % (plot_interval * 2) == 0 or epoch == n_epochs))\n",
    "        \n",
    "        if plot_this_epoch:\n",
    "            with torch.no_grad():\n",
    "                # Plot reconstructed training images\n",
    "                z_train = rep(indices_train.to(device))\n",
    "                reconstructions_train = model(z_train)\n",
    "                reconstructions_train = reconstructions_train.cpu()\n",
    "                plot_images(reconstructions_train, labels_train.cpu(), \"Reconstructed Train Images\", epoch=epoch)\n",
    "                \n",
    "                # Plot reconstructed test images\n",
    "                z_test = test_rep(indices_test.to(device))\n",
    "                reconstructions_test = model(z_test)\n",
    "                reconstructions_test = reconstructions_test.cpu()\n",
    "                plot_images(reconstructions_test, labels_test.cpu(), \"Reconstructed Test Images\", epoch=epoch)\n",
    "\n",
    "                # Plot GMM component reconstructions (only after GMM is fitted)\n",
    "                if epoch >= first_epoch_gmm and gmm is not None:\n",
    "                    plot_gmm_images(\n",
    "                        model, gmm, \"GMM Component Means (by weight)\",\n",
    "                        epoch=epoch, top_n=n_components, device=device\n",
    "                    )\n",
    "                    plot_gmm_samples(\n",
    "                        model, gmm, \"Generated Images from GMM Samples\",\n",
    "                        n_samples=n_components, epoch=epoch, device=device\n",
    "                    )\n",
    "\n",
    "        # Plot latent space visualizations (reduced frequency for weak GPUs)\n",
    "        if gmm is not None and verbose:\n",
    "            if epoch % plot_interval == 0 or epoch == n_epochs or epoch == 1 or epoch == first_epoch_gmm:\n",
    "                with torch.no_grad():\n",
    "                    z_train = rep.z.detach()\n",
    "                    z_test = test_rep.z.detach()\n",
    "                    labels_train = torch.tensor([label for _, _, label in indexed_train_dataset])\n",
    "                    labels_test = torch.tensor([label for _, _, label in indexed_test_dataset])\n",
    "\n",
    "                    label_names = [\n",
    "                        \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                        \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "                    ]\n",
    "\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='pca', \n",
    "                                    title=\"Latent Space - PCA\",\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "                    # Skip kernel PCA for weak GPUs\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='tsne', \n",
    "                                    perplexity=30,\n",
    "                                    n_iter=1000,\n",
    "                                    title=\"Latent Space - t-SNE\",\n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='umap',\n",
    "                                    n_neighbors=20,\n",
    "                                    n_components=2,\n",
    "                                    min_dist=0.01,\n",
    "                                    title=\"Latent Space - UMAP\",\n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "        elif gmm is not None and not verbose:\n",
    "            # Only show final latent space for non-verbose mode\n",
    "            if epoch == n_epochs:\n",
    "                with torch.no_grad():\n",
    "                    z_train = rep.z.detach()\n",
    "                    z_test = test_rep.z.detach()\n",
    "                    labels_train = torch.tensor([label for _, _, label in indexed_train_dataset])\n",
    "                    labels_test = torch.tensor([label for _, _, label in indexed_test_dataset])\n",
    "\n",
    "                    label_names = [\n",
    "                        \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                        \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "                    ]\n",
    "\n",
    "                    visualizer.visualize(z_train, labels_train, z_test, labels_test, gmm,\n",
    "                                    method='pca', \n",
    "                                    title=\"Final Latent Space - PCA\",\n",
    "                                    label_names=label_names,\n",
    "                                    epoch=epoch)\n",
    "        \n",
    "    if verbose:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {str(timedelta(seconds=int(total_time)))}\")\n",
    "        print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"Final test loss: {test_losses[-1]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Training completed: {train_losses[-1]:.4f} train, {test_losses[-1]:.4f} test\")\n",
    "        \n",
    "    return decoder_model, rep, test_rep\n",
    "\n",
    "# Initialize components\n",
    "nsample_train = len(indexed_train_dataset)\n",
    "nsample_test = len(indexed_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GMM Parameters\n",
    "n_features = 5\n",
    "n_components = 20\n",
    "covariance_type = 'diag'\n",
    "init_params = 'kmeans'\n",
    "max_iter = 1000\n",
    "tol = 1e-4\n",
    "warm_start = True\n",
    "verbose = False\n",
    "verbose_interval = 10\n",
    "n_init = 1\n",
    "\n",
    "\n",
    "\n",
    "# Representation Layer Parameters\n",
    "dist = 'uniform_ball'\n",
    "dist_options_train = {\n",
    "    \"n_samples\": nsample_train,\n",
    "    \"dim\": n_features,\n",
    "    \"radius\": 0.1,\n",
    "}\n",
    "dist_options_test = {\n",
    "    \"n_samples\": nsample_test,\n",
    "    \"dim\": n_features,\n",
    "    \"radius\": 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "# Decoder Parameters\n",
    "decoder_hidden_dims = [128, 64, 32]\n",
    "decoder_output_channels = 1\n",
    "decoder_output_size = (28, 28)\n",
    "decoder_activation = 'leaky_relu'\n",
    "decoder_final_activation = 'sigmoid'\n",
    "decoder_use_batch_norm = True\n",
    "decoder_dropout_rate = 0.1\n",
    "decoder_init_size = (7, 7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 200\n",
    "first_epoch_gmm = 50\n",
    "refit_gmm_interval = 100\n",
    "lambda_gmm = 1.0\n",
    "\n",
    "# Representation Layer Optimizer\n",
    "rep_lr = 0.01\n",
    "\n",
    "# Decoder Optimizer\n",
    "decoder_lr = 0.001\n",
    "\n",
    "\n",
    "# Plotting Parameters\n",
    "plot_interval = 50\n",
    "\n",
    "# Create the representation layer\n",
    "rep = RepresentationLayer(dist=dist, dist_options=dist_options_train, device=device)\n",
    "test_rep = RepresentationLayer(dist=dist, dist_options=dist_options_test, device=device)\n",
    "\n",
    "\n",
    "# Create GMM\n",
    "gmm = GaussianMixture(\n",
    "    n_features=n_features, \n",
    "    n_components=n_components, \n",
    "    covariance_type=covariance_type, \n",
    "    init_params=init_params,\n",
    "    device=device, \n",
    "    random_state=RANDOM_STATE, \n",
    "    verbose=False, \n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    n_init=n_init,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "# Create the decoder\n",
    "decoder = ConvDecoder(\n",
    "    latent_dim=n_features,\n",
    "    hidden_dims=decoder_hidden_dims,\n",
    "    output_channels=decoder_output_channels,\n",
    "    output_size=decoder_output_size,\n",
    "    use_batch_norm=decoder_use_batch_norm,\n",
    "    activation=decoder_activation,\n",
    "    final_activation=decoder_final_activation,\n",
    "    dropout_rate=decoder_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# Create the full model\n",
    "decoder_model = DGD(decoder, rep, gmm)\n",
    "\n",
    "\n",
    "# Count and print parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print parameter counts\n",
    "decoder_params = count_parameters(decoder)\n",
    "rep_params = count_parameters(rep)\n",
    "test_rep_params = count_parameters(test_rep)\n",
    "\n",
    "print(f\"Decoder parameters: {decoder_params:,} ({decoder_params / 1e6:.2f}M)\")\n",
    "print(f\"Train representation parameters: {rep_params:,} ({rep_params / 1e6:.2f}M)\")\n",
    "print(f\"Test representation parameters: {test_rep_params:,} ({test_rep_params / 1e6:.2f}M)\")\n",
    "print(f\"Total trainable parameters: {decoder_params + rep_params + test_rep_params:,} ({(decoder_params + rep_params + test_rep_params) / 1e6:.2f}M)\")\n",
    "\n",
    "# Analyze parameter distribution\n",
    "print(\"\\nParameter distribution:\")\n",
    "print(f\"Decoder: {decoder_params / (decoder_params + rep_params) * 100:.1f}%\")\n",
    "print(f\"Train rep: {rep_params / (decoder_params + rep_params) * 100:.1f}%\\n\")\n",
    "\n",
    "\n",
    "# Initialize optimizers\n",
    "decoder_optimizer = torch.optim.AdamW(\n",
    "    decoder.parameters(),\n",
    "    lr=decoder_lr,\n",
    ")\n",
    "trainrep_optimizer = torch.optim.AdamW(\n",
    "    rep.parameters(),\n",
    "    lr=rep_lr,\n",
    ")\n",
    "testrep_optimizer = torch.optim.AdamW(\n",
    "    test_rep.parameters(),\n",
    "    lr=rep_lr,\n",
    ")\n",
    "\n",
    "optimizers = [decoder_optimizer, trainrep_optimizer, testrep_optimizer]\n",
    "\n",
    "visualizer = LatentSpaceVisualizer()\n",
    "\n",
    "trained_model, trained_rep, trained_test_rep = train_model(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    decoder_model,\n",
    "    optimizers,\n",
    "    test_rep=test_rep,\n",
    "    n_epochs=epochs,\n",
    "    first_epoch_gmm=first_epoch_gmm,\n",
    "    refit_gmm_interval=refit_gmm_interval,\n",
    "    lambda_gmm=lambda_gmm,\n",
    "    device=device,\n",
    "    plot_interval=plot_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for weak GPU support\n",
    "use_subset_mode = use_small_dataset  # Use the existing variable\n",
    "verbose_mode = True  # Set to False for compact output\n",
    "\n",
    "# Run training with new parameters\n",
    "trained_model, trained_rep, trained_test_rep = train_model(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    decoder_model,\n",
    "    optimizers,\n",
    "    test_rep=test_rep,\n",
    "    n_epochs=epochs,\n",
    "    first_epoch_gmm=first_epoch_gmm,\n",
    "    refit_gmm_interval=refit_gmm_interval,\n",
    "    lambda_gmm=lambda_gmm,\n",
    "    device=device,\n",
    "    plot_interval=plot_interval,\n",
    "    use_subset=use_subset_mode,  # New parameter for subset indication\n",
    "    verbose=verbose_mode         # New parameter for detailed output\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
