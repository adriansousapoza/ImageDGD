# Default configuration for ImageDGD training
# This configuration provides reasonable defaults for production training

# General settings
random_seed: 42
experiment_name: "ImageDGD_Default"
description: "Default ImageDGD training configuration"

# ClearML configuration
clearml:
  enabled: false
  project_name: "ImageDGD"
  task_name: "Default_Training"
  tags: ["default", "production"]


# Data configuration
data:
  dataset_name: "FashionMNIST"
  root_dir: "./data"
  batch_size: 64
  num_workers: 1
  pin_memory: true
  total_subset_fraction: 1.0
  test_split: 0.5
  normalize: true
  download: true
  shuffle_train: true
  shuffle_test: false
  
  # Data transforms
  transforms:
    train: ['ToTensor', 'Normalize']
    test: ['ToTensor', 'Normalize']
  
  # Class names for FashionMNIST
  class_names:
    - 'Ankle boot'
    - 'Bag'
    - 'Coat'
    - 'Dress'
    - 'Pullover'
    - 'Sandal'
    - 'Shirt'
    - 'Sneaker'
    - 'T-shirt/top'
    - 'Trouser'
  
# Model configuration
model:
  # Representation layer configuration
  representation:
    n_features: 8
    distribution: "uniform_ball"  # Options: normal, uniform, uniform_ball, uniform_sphere, laplace, student_t, logistic, hyperbolic, zeros, pca
    dist_params: {'radius': 0.1}
    # Distribution-specific parameters examples:
    # uniform_ball: {'radius': 0.1}
    # normal: {'mean': 0, 'cov': 1}
    # pca: {'whiten': false, 'svd_solver': 'auto'}
  
  # Decoder configuration
  decoder:
    hidden_dims: [64, 32, 16]
    output_channels: 1
    output_size: [28, 28]
    init_size: [4, 4]
    kernel_size: 3
    stride: 2
    padding: 1
    output_padding: 1
    bias: true
    normalization: "batch"
    activation: "leaky_relu"
    final_activation: "sigmoid"
    dropout_rate: 0.0
    upsampling_mode: "transpose"
    use_spectral_norm: false
    use_self_attention: false
    attention_resolution: 32
  
  # GMM configuration
  gmm:
    n_components: 20
    n_features: null # If null, uses representation n_features
    covariance_type: "spherical"
    max_iter: 1000
    tol: 1e-4
    reg_covar: 1e-6
    n_init: 1
    init_means: "kmeans"
    init_weights: "uniform"
    init_covariances: "empirical"
    warm_start: true
    cem: true
    weight_concentration_prior: null
    mean_prior: null
    mean_precision_prior: null
    covariance_prior: null
    degrees_of_freedom_prior: null
    verbose: false
    verbose_interval: 10

# Training configuration
training:
  epochs: 50
  device: "auto"
  
  # Early stopping
  early_stopping_patience: 10
  
  # GMM training schedule
  first_epoch_gmm: 0
  refit_gmm_interval: 50
  lambda_gmm: 1.0
  final_gmm_fit: true
  
  # Optimizer configuration (AdamW)
  optimizer:
    decoder:
      lr: 0.005
      weight_decay: 0.01
      betas: [0.9, 0.999]
      eps: 1e-8
      amsgrad: false
    representation:
      lr: 0.05
      weight_decay: 0.0
      betas: [0.9, 0.999]
      eps: 1e-8
      amsgrad: false
  
  # Learning rate scheduler
  lr_scheduler:
    enabled: true
    warmup_epochs: 10  # Linear warmup for this many epochs
    eta_min: 1e-6  # Minimum learning rate for cosine annealing
